{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496a6fb5",
   "metadata": {},
   "source": [
    "## Financial Indices Data ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041e6ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: test\n",
      "Downloading data for S&P 500 (^GSPC)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imesh\\AppData\\Local\\Temp\\ipykernel_25420\\2154956733.py:32: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\imesh\\AppData\\Local\\Temp\\ipykernel_25420\\2154956733.py:32: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(ticker, start=start_date, end=end_date)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to test\\s&p_500_daily.csv\n",
      "\n",
      "Downloading data for NASDAQ (^IXIC)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\imesh\\AppData\\Local\\Temp\\ipykernel_25420\\2154956733.py:32: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(ticker, start=start_date, end=end_date)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to test\\nasdaq_daily.csv\n",
      "\n",
      "Downloading data for FTSE 100 (^FTSE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to test\\ftse_100_daily.csv\n",
      "\n",
      "--- Financial data collection complete! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of tickers for the indices you need \n",
    "tickers = {\n",
    "    \"S&P 500\": \"^GSPC\",\n",
    "    \"NASDAQ\": \"^IXIC\",\n",
    "    \"FTSE 100\": \"^FTSE\"\n",
    "}\n",
    "\n",
    "# Set the date range for the data \n",
    "# We use today's date for the end since 2025 is in the future.\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = pd.Timestamp.now().strftime('%Y-%m-%d') # Gets today's date\n",
    "\n",
    "# Path to save the test\n",
    "output_folder = \"test\"\n",
    "\n",
    "# --- Data Collection ---\n",
    "# Create the data folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Created folder: {output_folder}\")\n",
    "\n",
    "# Loop through each ticker, download data, and save it\n",
    "for name, ticker in tickers.items():\n",
    "    print(f\"Downloading data for {name} ({ticker})...\")\n",
    "    \n",
    "    # Download the data using yfinance\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    \n",
    "    if not data.empty:\n",
    "        # Define the output file path\n",
    "        file_name = f\"{name.replace(' ', '_').lower()}_daily.csv\"\n",
    "        output_path = os.path.join(output_folder, file_name)\n",
    "        \n",
    "        # Save the data to a CSV file\n",
    "        data.to_csv(output_path)\n",
    "        print(f\"Successfully saved data to {output_path}\\n\")\n",
    "    else:\n",
    "        print(f\"Could not download data for {name}. It might be delisted or the ticker is wrong.\\n\")\n",
    "\n",
    "print(\"--- Financial data collection complete! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd809aa",
   "metadata": {},
   "source": [
    "##  Macroeconomic Indicators ðŸ¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b650a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading macroeconomic data from FRED...\n",
      "Successfully saved VIX data to test\\macro_vix.csv\n",
      "Successfully saved GDP data to test\\macro_gdp.csv\n",
      "Successfully saved UnemploymentRate data to test\\macro_unemploymentrate.csv\n",
      "Successfully saved Inflation_CPI data to test\\macro_inflation_cpi.csv\n",
      "\n",
      "--- Macroeconomic data collection complete! ---\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader.data as web\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the series IDs from FRED for the indicators you need \n",
    "# Note: These are official codes from the FRED website.\n",
    "series_ids = {\n",
    "    \"VIX\": \"VIXCLS\",                     # Volatility Index (Daily)\n",
    "    \"GDP\": \"GDP\",                        # Gross Domestic Product (Quarterly)\n",
    "    \"UnemploymentRate\": \"UNRATE\",        # Unemployment Rate (Monthly)\n",
    "    \"Inflation_CPI\": \"CPIAUCSL\"          # Consumer Price Index / Inflation (Monthly)\n",
    "}\n",
    "\n",
    "# Set the date range\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Path to save the data\n",
    "output_folder = \"test\"\n",
    "\n",
    "# --- Data Collection ---\n",
    "print(\"Downloading macroeconomic data from FRED...\")\n",
    "\n",
    "for name, series_id in series_ids.items():\n",
    "    try:\n",
    "        # Download data from FRED\n",
    "        data = web.DataReader(series_id, 'fred', start_date, end_date)\n",
    "        \n",
    "        # Define the output file path\n",
    "        output_path = os.path.join(output_folder, f\"macro_{name.lower()}.csv\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        data.to_csv(output_path)\n",
    "        print(f\"Successfully saved {name} data to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not download data for {name}. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Macroeconomic data collection complete! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d40be",
   "metadata": {},
   "source": [
    "## Sentiment Data (Text) ðŸ“°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41de4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "import time\n",
    "\n",
    "# --- IMPORTANT NOTE ---\n",
    "# This script will ONLY work if you have a paid NewsAPI plan that allows\n",
    "# access to historical articles. The free plan is limited to the last 30 days.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Replace with your actual API key from a paid plan\n",
    "api_key = 'YOUR_PAID_API_KEY' \n",
    "\n",
    "# The keyword or phrase to search for\n",
    "search_query = 'stock market OR finance OR economy'\n",
    "\n",
    "# Define the full date range for your research\n",
    "start_date = '2015-01-01'\n",
    "end_date = pd.Timestamp.now().strftime('%Y-%m-%d') # Today's date\n",
    "\n",
    "# Path to save the final CSV file\n",
    "output_folder = \"data\"\n",
    "output_filename = os.path.join(output_folder, \"news_headlines_2015_to_2025.csv\")\n",
    "\n",
    "# --- Initialization ---\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "all_articles_list = []\n",
    "\n",
    "# Generate a list of start/end dates for each month in your range\n",
    "# This is how we get around the API's limitations for large requests.\n",
    "date_ranges = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "\n",
    "print(f\"Starting data collection from {start_date} to {end_date}...\")\n",
    "\n",
    "# --- Main Loop to Fetch Historical Data ---\n",
    "for month_start_date in date_ranges:\n",
    "    month_end_date = (month_start_date + pd.offsets.MonthEnd(1)).strftime('%Y-%m-%d')\n",
    "    month_start_date_str = month_start_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Fetching news for: {month_start_date_str} to {month_end_date}...\")\n",
    "    \n",
    "    try:\n",
    "        # Make the request for the current month\n",
    "        response = newsapi.get_everything(\n",
    "            q=search_query,\n",
    "            language='en',\n",
    "            from_param=month_start_date_str,\n",
    "            to=month_end_date,\n",
    "            sort_by='publishedAt',\n",
    "            page_size=100 # Max results per page\n",
    "        )\n",
    "        \n",
    "        if response['status'] == 'ok':\n",
    "            # Add the fetched articles to our master list\n",
    "            for article in response['articles']:\n",
    "                all_articles_list.append({\n",
    "                    'date': article['publishedAt'],\n",
    "                    'title': article['title'],\n",
    "                    'source': article['source']['name']\n",
    "                })\n",
    "            print(f\"  > Found {response['totalResults']} articles for this month.\")\n",
    "        else:\n",
    "            print(f\"  > Error fetching data for this month: {response.get('message')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  > An error occurred: {e}\")\n",
    "\n",
    "    # Be respectful to the API server, wait 1 second between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "if all_articles_list:\n",
    "    # Convert the list of articles into a pandas DataFrame\n",
    "    articles_df = pd.DataFrame(all_articles_list)\n",
    "    \n",
    "    # Clean up the date column\n",
    "    articles_df['date'] = pd.to_datetime(articles_df['date']).dt.tz_localize(None)\n",
    "    \n",
    "    # Save the final DataFrame to a CSV file\n",
    "    articles_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSuccessfully collected and saved {len(articles_df)} articles to {output_filename}\")\n",
    "else:\n",
    "    print(\"\\nNo articles were collected. Please check your API key and plan.\")\n",
    "\n",
    "print(\"\\n--- Historical news data collection complete! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe8869",
   "metadata": {},
   "source": [
    "##  Geopolitical & Crisis Events Data ðŸ—“ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde7ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Starting GDELT data collection from 2024-01-01 to 2024-01-07...\n",
      "Searching 1 weekly periods...\n",
      "Note: This is a test run with a smaller date range to avoid API limitations\n",
      "Week 1/1: Searching events for 2024-01-07 to 2024-01-13...\n",
      "Starting GDELT data collection from 2024-01-01 to 2024-01-07...\n",
      "Searching 1 weekly periods...\n",
      "Note: This is a test run with a smaller date range to avoid API limitations\n",
      "Week 1/1: Searching events for 2024-01-07 to 2024-01-13...\n",
      "  > Found 7300 relevant events.\n",
      "  > Found 7300 relevant events.\n",
      "Available columns: ['DATE', 'SourceCommonName', 'DocumentIdentifier', 'V2Tone', 'Themes', 'Locations']\n",
      "\n",
      "Successfully collected and saved 7300 total events to data\\gdelt_crisis_events_test.csv\n",
      "Sample of first few rows:\n",
      "             DATE            SourceCommonName  \\\n",
      "0  20240107234500                    wfmj.com   \n",
      "1  20240107234500           politicalwire.com   \n",
      "2  20240107234500  northerndailyleader.com.au   \n",
      "3  20240107234500       idahostatejournal.com   \n",
      "4  20240107234500              times-news.com   \n",
      "\n",
      "                                  DocumentIdentifier  \\\n",
      "0  https://www.wfmj.com/story/50305856/explainer-...   \n",
      "1  https://politicalwire.com/2024/01/07/pentagons...   \n",
      "2  https://www.northerndailyleader.com.au/story/8...   \n",
      "3  https://www.idahostatejournal.com/news/nationa...   \n",
      "4  https://www.times-news.com/news/blinken-meets-...   \n",
      "\n",
      "                                              V2Tone  \\\n",
      "0  -3.46020761245675,0.576701268742791,4.03690888...   \n",
      "1  1.20481927710843,3.6144578313253,2.40963855421...   \n",
      "2  1.99335548172757,3.48837209302326,1.4950166112...   \n",
      "3  -1.31233595800525,1.5748031496063,2.8871391076...   \n",
      "4  -2.23642172523962,2.95527156549521,5.191693290...   \n",
      "\n",
      "                                              Themes  \\\n",
      "0  TAX_FNCACT;TAX_FNCACT_WRITER;CRISISLEX_C07_SAF...   \n",
      "1  TAX_ETHNICITY;TAX_ETHNICITY_UKRAINIAN;TAX_WORL...   \n",
      "2  USPEC_POLICY1;EPU_POLICY;EPU_POLICY_SPENDING;U...   \n",
      "3  TAX_FNCACT;TAX_FNCACT_OFFICIALS;TAX_FNCACT_FED...   \n",
      "4  TAX_FNCACT;TAX_FNCACT_SECRETARY;TAX_FNCACT_SEC...   \n",
      "\n",
      "                                           Locations  \n",
      "0  2#Oregon, United States#US#USOR#44.5672#-122.1...  \n",
      "1                           1#Ukraine#UP#UP#49#32#UP  \n",
      "2  4#Gleneagle, Queensland, Australia#AS#AS04#-27...  \n",
      "3  2#New York, United States#US#USNY#42.1497#-74....  \n",
      "4  1#Jordan#JO#JO#31#36#JO;4#Red Sea, Djibouti (G...  \n",
      "\n",
      "--- GDELT crisis event collection complete! ---\n",
      "Available columns: ['DATE', 'SourceCommonName', 'DocumentIdentifier', 'V2Tone', 'Themes', 'Locations']\n",
      "\n",
      "Successfully collected and saved 7300 total events to data\\gdelt_crisis_events_test.csv\n",
      "Sample of first few rows:\n",
      "             DATE            SourceCommonName  \\\n",
      "0  20240107234500                    wfmj.com   \n",
      "1  20240107234500           politicalwire.com   \n",
      "2  20240107234500  northerndailyleader.com.au   \n",
      "3  20240107234500       idahostatejournal.com   \n",
      "4  20240107234500              times-news.com   \n",
      "\n",
      "                                  DocumentIdentifier  \\\n",
      "0  https://www.wfmj.com/story/50305856/explainer-...   \n",
      "1  https://politicalwire.com/2024/01/07/pentagons...   \n",
      "2  https://www.northerndailyleader.com.au/story/8...   \n",
      "3  https://www.idahostatejournal.com/news/nationa...   \n",
      "4  https://www.times-news.com/news/blinken-meets-...   \n",
      "\n",
      "                                              V2Tone  \\\n",
      "0  -3.46020761245675,0.576701268742791,4.03690888...   \n",
      "1  1.20481927710843,3.6144578313253,2.40963855421...   \n",
      "2  1.99335548172757,3.48837209302326,1.4950166112...   \n",
      "3  -1.31233595800525,1.5748031496063,2.8871391076...   \n",
      "4  -2.23642172523962,2.95527156549521,5.191693290...   \n",
      "\n",
      "                                              Themes  \\\n",
      "0  TAX_FNCACT;TAX_FNCACT_WRITER;CRISISLEX_C07_SAF...   \n",
      "1  TAX_ETHNICITY;TAX_ETHNICITY_UKRAINIAN;TAX_WORL...   \n",
      "2  USPEC_POLICY1;EPU_POLICY;EPU_POLICY_SPENDING;U...   \n",
      "3  TAX_FNCACT;TAX_FNCACT_OFFICIALS;TAX_FNCACT_FED...   \n",
      "4  TAX_FNCACT;TAX_FNCACT_SECRETARY;TAX_FNCACT_SEC...   \n",
      "\n",
      "                                           Locations  \n",
      "0  2#Oregon, United States#US#USOR#44.5672#-122.1...  \n",
      "1                           1#Ukraine#UP#UP#49#32#UP  \n",
      "2  4#Gleneagle, Queensland, Australia#AS#AS04#-27...  \n",
      "3  2#New York, United States#US#USNY#42.1497#-74....  \n",
      "4  1#Jordan#JO#JO#31#36#JO;4#Red Sea, Djibouti (G...  \n",
      "\n",
      "--- GDELT crisis event collection complete! ---\n"
     ]
    }
   ],
   "source": [
    "import gdelt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- IMPORTANT NOTE ---\n",
    "# This is a heavy data request and can take a long time to run (possibly hours).\n",
    "# It will search for events month-by-month over a 10+ year period.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set up GDELT version 2\n",
    "gd = gdelt.gdelt(version=2)\n",
    "\n",
    "# Define a smaller date range for testing (GDELT has strict limitations)\n",
    "# Let's start with just a few recent months to test the approach\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2024-01-07\"  # Just 3 months for testing\n",
    "\n",
    "# Path to save the final CSV file\n",
    "output_folder = \"data\"\n",
    "output_filename = os.path.join(output_folder, \"gdelt_crisis_events_test.csv\")\n",
    "\n",
    "# --- Initialization ---\n",
    "all_results = []\n",
    "# Generate weekly date ranges instead of monthly (GDELT prefers smaller ranges)\n",
    "date_ranges = pd.date_range(start=start_date, end=end_date, freq='W')\n",
    "\n",
    "print(f\"Starting GDELT data collection from {start_date} to {end_date}...\")\n",
    "print(f\"Searching {len(date_ranges)} weekly periods...\")\n",
    "print(f\"Note: This is a test run with a smaller date range to avoid API limitations\")\n",
    "\n",
    "# --- Main Loop to Fetch Historical Data ---\n",
    "for i, week_start_date in enumerate(date_ranges):\n",
    "    # Calculate the end of the current week (7 days later)\n",
    "    week_end_date = (week_start_date + pd.Timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "    week_start_date_str = week_start_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Week {i+1}/{len(date_ranges)}: Searching events for {week_start_date_str} to {week_end_date}...\")\n",
    "    \n",
    "    try:\n",
    "        # Search GDELT for the current week using a simple date range\n",
    "        # Using 'gkg' table for Global Knowledge Graph data\n",
    "        results_df = gd.Search(\n",
    "            date=[week_start_date_str, week_end_date], \n",
    "            table='gkg',\n",
    "            output='df'\n",
    "        )\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            # Filter for economic/financial themes if the Themes column exists\n",
    "            if 'Themes' in results_df.columns:\n",
    "                # Filter for rows containing economic/financial themes\n",
    "                economic_filter = results_df['Themes'].str.contains(\n",
    "                    'ECON_STOCKMARKET|FINANCIAL|CRISIS|UNREST|MARKET|ECONOMY', \n",
    "                    case=False, \n",
    "                    na=False\n",
    "                )\n",
    "                results_df = results_df[economic_filter]\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                print(f\"  > Found {len(results_df)} relevant events.\")\n",
    "                all_results.append(results_df)\n",
    "            else:\n",
    "                print(\"  > No relevant economic events found for this week.\")\n",
    "        else:\n",
    "            print(\"  > No events found for this week.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  > An error occurred for this week: {e}\")\n",
    "        # Continue with next week even if this one fails\n",
    "        continue\n",
    "        \n",
    "    # Wait 2 seconds to be respectful to the GDELT servers\n",
    "    time.sleep(2)\n",
    "\n",
    "# --- Combine, Clean, and Save Results ---\n",
    "if all_results:\n",
    "    # Combine all the weekly DataFrames into one large DataFrame\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Let's select a few of the most useful columns to keep the file size manageable\n",
    "    columns_to_keep = [\n",
    "        'DATE',\n",
    "        'SourceCommonName',\n",
    "        'DocumentIdentifier',\n",
    "        'V2Tone', # This is very useful - it includes sentiment scores\n",
    "        'Themes',\n",
    "        'Locations'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns actually exist in the data\n",
    "    available_columns = [col for col in columns_to_keep if col in final_df.columns]\n",
    "    print(f\"Available columns: {available_columns}\")\n",
    "    \n",
    "    # Filter for only the columns we need that actually exist\n",
    "    final_df_cleaned = final_df[available_columns]\n",
    "    \n",
    "    # Save the final DataFrame to a CSV file\n",
    "    final_df_cleaned.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully collected and saved {len(final_df_cleaned)} total events to {output_filename}\")\n",
    "    print(f\"Sample of first few rows:\")\n",
    "    print(final_df_cleaned.head())\n",
    "else:\n",
    "    print(\"\\nNo events were collected overall. Try adjusting your date range or check the API status.\")\n",
    "\n",
    "print(\"\\n--- GDELT crisis event collection complete! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5136c21",
   "metadata": {},
   "source": [
    "## Advanced GDELT Data Filtering for Financial Research ðŸŽ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c5e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 7300 events\n",
      "Date range: 20240107234500 to 20240113234500\n",
      "\n",
      "After financial theme filtering: 4964 events (68.0% of original)\n",
      "Successfully parsed sentiment for 4964 events\n",
      "\n",
      "Event categorization:\n",
      "High impact (key themes + major markets): 3323 events\n",
      "Medium impact (other themes + major markets): 1463 events\n",
      "Other financial events: 1246 events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imesh\\AppData\\Local\\Temp\\ipykernel_24436\\2711434923.py:100: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  high_impact_events = financial_df[high_priority_filter & major_market_filter].copy()\n",
      "C:\\Users\\imesh\\AppData\\Local\\Temp\\ipykernel_24436\\2711434923.py:101: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  medium_impact_events = financial_df[medium_priority_filter & major_market_filter].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FILTERING COMPLETE ---\n",
      "Final dataset for forecasting: 4786 events\n",
      "Saved to: data/gdelt_financial_filtered.csv\n",
      "\n",
      "Sample of high-impact events:\n",
      "\n",
      "Date: 20240107234500\n",
      "Source: northerndailyleader.com.au\n",
      "Avg Tone: 1.99335548172757\n",
      "Key Themes: USPEC_POLICY1;EPU_POLICY;EPU_POLICY_SPENDING;UNGP_FORESTS_RIVERS_OCEANS;AFFECT;TAX_FNCACT;TAX_FNCACT_CHILD;USPEC_POLITICS_GENERAL1;IDEOLOGY;ECON_DEBT;WB_1104_MACROECONOMIC_VULNERABILITY_AND_DEBT;WB_45...\n",
      "Locations: 4#Gleneagle, Queensland, Australia#AS#AS04#-27.9333#152.983#-1575354...\n",
      "\n",
      "Date: 20240107234500\n",
      "Source: idahostatejournal.com\n",
      "Avg Tone: -1.31233595800525\n",
      "Key Themes: TAX_FNCACT;TAX_FNCACT_OFFICIALS;TAX_FNCACT_FEDERAL_OFFICIALS;CRISISLEX_C04_LOGISTICS_TRANSPORT;MANMADE_DISASTER_IMPLIED;WB_1921_PRIVATE_SECTOR_DEVELOPMENT;WB_405_BUSINESS_CLIMATE;WB_2531_INSPECTIONS_L...\n",
      "Locations: 2#New York, United States#US#USNY#42.1497#-74.9384#NY;1#South Korea#KS#KS#37#127.5#KS;3#Portland, Or...\n",
      "\n",
      "Date: 20240107234500\n",
      "Source: times-news.com\n",
      "Avg Tone: -2.23642172523962\n",
      "Key Themes: TAX_FNCACT;TAX_FNCACT_SECRETARY;TAX_FNCACT_SECRETARY_OF_STATE;TAX_ETHNICITY;TAX_ETHNICITY_ARAB;ARMEDCONFLICT;EPU_CATS_NATIONAL_SECURITY;TAX_TERROR_GROUP;TAX_TERROR_GROUP_HAMAS;TAX_POLITICAL_PARTY;TAX_...\n",
      "Locations: 1#Jordan#JO#JO#31#36#JO;4#Red Sea, Djibouti (General), Djibouti#DJ#DJ00#19#39.5#-2037160;4#Istanbul,...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the collected GDELT data\n",
    "df = pd.read_csv('data/gdelt_crisis_events_test.csv')\n",
    "\n",
    "print(f\"Original dataset size: {len(df)} events\")\n",
    "print(f\"Date range: {df['DATE'].min()} to {df['DATE'].max()}\")\n",
    "\n",
    "# --- FINANCIAL-SPECIFIC THEME FILTERING ---\n",
    "# Define highly relevant financial/economic themes for forecasting\n",
    "high_priority_themes = [\n",
    "    'ECON_STOCKMARKET',           # Direct stock market events\n",
    "    'ECON_INFLATION',             # Inflation events\n",
    "    'ECON_RECESSION',             # Recession indicators\n",
    "    'ECON_GDP',                   # GDP-related events\n",
    "    'ECON_UNEMPLOYMENT',          # Employment data\n",
    "    'ECON_INTEREST_RATE',         # Interest rate changes\n",
    "    'ECON_CURRENCY',              # Currency events\n",
    "    'WB_.*FINANCIAL.*',           # World Bank financial themes\n",
    "    'WB_.*ECONOMIC.*',            # World Bank economic themes\n",
    "    'WB_.*MONETARY.*',            # Monetary policy\n",
    "    'EPU_POLICY',                 # Economic Policy Uncertainty\n",
    "    'EPU_ECONOMY',                # Economic uncertainty\n",
    "    'CRISISLEX.*FINANCIAL',       # Financial crisis lexicon\n",
    "]\n",
    "\n",
    "medium_priority_themes = [\n",
    "    'ECON_DEBT',                  # Debt-related events\n",
    "    'ECON_TRADE',                 # Trade events\n",
    "    'ECON_DEVELOPMENTORGS',       # Development organizations\n",
    "    'WB_.*BUSINESS.*',            # Business environment\n",
    "    'WB_.*INVESTMENT.*',          # Investment climate\n",
    "    'WB_.*BANKING.*',             # Banking sector\n",
    "    'MANMADE_DISASTER.*FINANCIAL', # Financial disasters\n",
    "    'AFFECT',                     # Market sentiment\n",
    "]\n",
    "\n",
    "# Create pattern strings for filtering\n",
    "high_priority_pattern = '|'.join(high_priority_themes)\n",
    "medium_priority_pattern = '|'.join(medium_priority_themes)\n",
    "\n",
    "# Apply filtering\n",
    "high_priority_filter = df['Themes'].str.contains(high_priority_pattern, case=False, na=False, regex=True)\n",
    "medium_priority_filter = df['Themes'].str.contains(medium_priority_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "# Combine filters\n",
    "financial_filter = high_priority_filter | medium_priority_filter\n",
    "\n",
    "# Apply the filter\n",
    "financial_df = df[financial_filter].copy()\n",
    "\n",
    "print(f\"\\nAfter financial theme filtering: {len(financial_df)} events ({len(financial_df)/len(df)*100:.1f}% of original)\")\n",
    "\n",
    "# --- SENTIMENT SCORING ---\n",
    "# Parse the V2Tone column (format: \"tone1,tone2,tone3,tone4,tone5,tone6,wordcount\")\n",
    "def parse_v2tone(v2tone_str):\n",
    "    \"\"\"Parse V2Tone string into individual sentiment components\"\"\"\n",
    "    try:\n",
    "        parts = str(v2tone_str).split(',')\n",
    "        if len(parts) >= 7:\n",
    "            return {\n",
    "                'tone_avg': float(parts[0]),           # Average tone (-100 to +100)\n",
    "                'tone_positive': float(parts[1]),      # Positive sentiment score\n",
    "                'tone_negative': float(parts[2]),      # Negative sentiment score  \n",
    "                'tone_polarity': float(parts[3]),      # Polarity (how extreme)\n",
    "                'tone_activity': float(parts[4]),      # Activity reference density\n",
    "                'tone_self_group': float(parts[5]),    # Self/group reference density\n",
    "                'word_count': int(parts[6])            # Total words in document\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Apply sentiment parsing\n",
    "sentiment_data = financial_df['V2Tone'].apply(parse_v2tone)\n",
    "sentiment_df = pd.json_normalize(sentiment_data.dropna())\n",
    "\n",
    "# Add sentiment scores back to financial_df\n",
    "valid_indices = sentiment_data.dropna().index\n",
    "for col in sentiment_df.columns:\n",
    "    financial_df.loc[valid_indices, col] = sentiment_df[col].values\n",
    "\n",
    "print(f\"Successfully parsed sentiment for {len(sentiment_df)} events\")\n",
    "\n",
    "# --- LOCATION FILTERING ---\n",
    "# Focus on major financial centers and markets\n",
    "major_financial_centers = [\n",
    "    'New York', 'London', 'Tokyo', 'Hong Kong', 'Singapore', 'Frankfurt', \n",
    "    'Zurich', 'Toronto', 'Sydney', 'Paris', 'Milan', 'Amsterdam',\n",
    "    'United States', 'United Kingdom', 'Germany', 'Japan', 'China',\n",
    "    'Switzerland', 'Canada', 'Australia', 'France', 'Italy'\n",
    "]\n",
    "\n",
    "# Create location filter\n",
    "location_pattern = '|'.join(major_financial_centers)\n",
    "major_market_filter = financial_df['Locations'].str.contains(location_pattern, case=False, na=False)\n",
    "\n",
    "# Split data into different priority levels\n",
    "high_impact_events = financial_df[high_priority_filter & major_market_filter].copy()\n",
    "medium_impact_events = financial_df[medium_priority_filter & major_market_filter].copy()\n",
    "other_financial_events = financial_df[~major_market_filter].copy()\n",
    "\n",
    "print(f\"\\nEvent categorization:\")\n",
    "print(f\"High impact (key themes + major markets): {len(high_impact_events)} events\")\n",
    "print(f\"Medium impact (other themes + major markets): {len(medium_impact_events)} events\")\n",
    "print(f\"Other financial events: {len(other_financial_events)} events\")\n",
    "\n",
    "# --- FINAL CLEANED DATASET ---\n",
    "# Focus on high and medium impact events for forecasting\n",
    "forecasting_dataset = pd.concat([high_impact_events, medium_impact_events], ignore_index=True)\n",
    "\n",
    "# Clean up columns and add derived features\n",
    "forecasting_dataset['date_parsed'] = pd.to_datetime(forecasting_dataset['DATE'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "forecasting_dataset['date_only'] = forecasting_dataset['date_parsed'].dt.date\n",
    "\n",
    "# Save the filtered dataset\n",
    "output_path = 'data/gdelt_financial_filtered.csv'\n",
    "forecasting_dataset.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n--- FILTERING COMPLETE ---\")\n",
    "print(f\"Final dataset for forecasting: {len(forecasting_dataset)} events\")\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n",
    "# Show sample of the most relevant events\n",
    "print(f\"\\nSample of high-impact events:\")\n",
    "if len(high_impact_events) > 0:\n",
    "    sample_events = high_impact_events.head(3)\n",
    "    for idx, row in sample_events.iterrows():\n",
    "        print(f\"\\nDate: {row['DATE']}\")\n",
    "        print(f\"Source: {row['SourceCommonName']}\")\n",
    "        print(f\"Avg Tone: {row.get('tone_avg', 'N/A')}\")\n",
    "        print(f\"Key Themes: {row['Themes'][:200]}...\")\n",
    "        print(f\"Locations: {row['Locations'][:100]}...\")\n",
    "else:\n",
    "    print(\"No high-impact events found in this sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd67489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THEME ANALYSIS ===\n",
      "Financial/Economic themes found in your data:\n",
      "--------------------------------------------------\n",
      "EPU_ECONOMY_HISTORIC                               : 2136 occurrences\n",
      "TAX_ECON_PRICE                                     : 1295 occurrences\n",
      "WB_2025_INVESTIGATION                              :  746 occurrences\n",
      "EPU_ECONOMY                                        :  682 occurrences\n",
      "WB_698_TRADE                                       :  622 occurrences\n",
      "WB_1920_FINANCIAL_SECTOR_DEVELOPMENT               :  494 occurrences\n",
      "ECON_STOCKMARKET                                   :  452 occurrences\n",
      "WB_1104_MACROECONOMIC_VULNERABILITY_AND_DEBT       :  411 occurrences\n",
      "WB_1484_EDUCATION_SKILLS_DEVELOPMENT_AND_LABOR_MARKET :  411 occurrences\n",
      "WB_405_BUSINESS_CLIMATE                            :  358 occurrences\n",
      "WB_2530_BUSINESS_ENVIRONMENT                       :  352 occurrences\n",
      "ECON_TAXATION                                      :  291 occurrences\n",
      "WB_855_LABOR_MARKETS                               :  284 occurrences\n",
      "WB_328_FINANCIAL_INTEGRITY                         :  268 occurrences\n",
      "ECON_WORLDCURRENCIES                               :  255 occurrences\n",
      "ECON_INFLATION                                     :  252 occurrences\n",
      "WB_442_INFLATION                                   :  240 occurrences\n",
      "DRUG_TRADE                                         :  236 occurrences\n",
      "WB_1973_FINANCIAL_RISK_REDUCTION                   :  194 occurrences\n",
      "EPU_CATS_FINANCIAL_REGULATION                      :  179 occurrences\n",
      "WB_2745_JOB_QUALITY_AND_LABOR_MARKET_PERFORMANCE   :  171 occurrences\n",
      "WB_450_DEBT                                        :  166 occurrences\n",
      "WB_471_ECONOMIC_GROWTH                             :  161 occurrences\n",
      "TAX_FNCACT_INVESTOR                                :  156 occurrences\n",
      "ECON_INTEREST_RATES                                :  156 occurrences\n",
      "ECON_DEBT                                          :  155 occurrences\n",
      "ECON_HOUSING_PRICES                                :  148 occurrences\n",
      "WB_332_CAPITAL_MARKETS                             :  147 occurrences\n",
      "WB_1654_ACTIVE_LABOR_MARKET_POLICIES               :  123 occurrences\n",
      "ECON_BITCOIN                                       :  119 occurrences\n",
      "ECON_WORLDCURRENCIES_DOLLARS                       :  117 occurrences\n",
      "ECON_WORLDCURRENCIES_DOLLAR                        :  115 occurrences\n",
      "ECON_ENTREPRENEURSHIP                              :  111 occurrences\n",
      "WB_336_NON_BANK_FINANCIAL_INSTITUTIONS             :  109 occurrences\n",
      "WB_439_MACROECONOMIC_AND_STRUCTURAL_POLICIES       :  100 occurrences\n",
      "ECON_DEVELOPMENTORGS                               :   96 occurrences\n",
      "WB_625_HEALTH_ECONOMICS_AND_FINANCE                :   77 occurrences\n",
      "ECON_UNIONS                                        :   72 occurrences\n",
      "WB_318_FINANCIAL_ARCHITECTURE_AND_BANKING          :   72 occurrences\n",
      "ECON_COST_OF_LIVING                                :   65 occurrences\n",
      "WB_1673_PASSIVE_LABOR_MARKETS_POLICIES             :   63 occurrences\n",
      "ECON_OILPRICE                                      :   57 occurrences\n",
      "WB_1234_BANKING_INSTITUTIONS                       :   56 occurrences\n",
      "TAX_FNCACT_ECONOMIST                               :   55 occurrences\n",
      "SLFID_ECONOMIC_DEVELOPMENT                         :   53 occurrences\n",
      "TAX_FNCACT_TRADERS                                 :   50 occurrences\n",
      "WB_772_TRADE_FACILITATION_AND_LOGISTICS            :   50 occurrences\n",
      "TAX_FNCACT_INVESTIGATOR                            :   47 occurrences\n",
      "ECON_BOYCOTT                                       :   46 occurrences\n",
      "ECON_CENTRALBANK                                   :   46 occurrences\n",
      "WB_1235_CENTRAL_BANKS                              :   46 occurrences\n",
      "EPU_CATS_DEBT_CEILING_GOV_SHUTDOWN                 :   43 occurrences\n",
      "TAX_FNCACT_ECONOMISTS                              :   43 occurrences\n",
      "WB_2601_TRADE_LINKAGES_SPILLOVERS_AND_CONNECTIVITY :   42 occurrences\n",
      "ECON_BUDGET_DEFICIT                                :   41 occurrences\n",
      "EPU_POLICY_CENTRAL_BANK                            :   41 occurrences\n",
      "ECON_CURRENCY_EXCHANGE_RATE                        :   38 occurrences\n",
      "ECON_SUBSIDIES                                     :   38 occurrences\n",
      "TAX_FNCACT_CHIEF_FINANCIAL_OFFICER                 :   36 occurrences\n",
      "TAX_FNCACT_BUSINESSMAN                             :   34 occurrences\n",
      "WB_2480_POST_CONFLICT_RECONSTRUCTION               :   33 occurrences\n",
      "WB_1070_ECONOMIC_GROWTH_POLICY                     :   32 occurrences\n",
      "WB_363_FINANCIAL_INCLUSION                         :   31 occurrences\n",
      "WB_162_TRANSPORT_ECONOMICS                         :   31 occurrences\n",
      "WB_341_INVESTMENT_FUNDS                            :   28 occurrences\n",
      "ECON_DEVELOPMENTORGS_WORLD_HEALTH_ORGANIZATION     :   27 occurrences\n",
      "ECON_DEVELOPMENTORGS_INTERNATIONAL_MONETARY_FUND   :   25 occurrences\n",
      "ECON_ELECTRICALGENERATION                          :   25 occurrences\n",
      "WB_410_BUSINESS_LAW_AND_REGULATION                 :   25 occurrences\n",
      "ECON_PRICEGOUGE                                    :   24 occurrences\n",
      "WB_1670_TRADE_UNIONS                               :   23 occurrences\n",
      "WB_1042_TRADEMARKS                                 :   23 occurrences\n",
      "SLFID_ECONOMIC_DEVELOPMENTAID                      :   23 occurrences\n",
      "WB_350_FINANCIAL_INFRASTRUCTURE_AND_REMITTANCES    :   22 occurrences\n",
      "EPU_CATS_TRADE_POLICY                              :   22 occurrences\n",
      "WB_775_TRADE_POLICY_AND_INTEGRATION                :   22 occurrences\n",
      "WB_853_FINANCIAL_LAWS_AND_REGULATIONS              :   21 occurrences\n",
      "WB_904_HOUSING_MARKETS                             :   20 occurrences\n",
      "ECON_ELECTRICALGRID                                :   20 occurrences\n",
      "WB_2111_COMPETITION_ECONOMICS                      :   20 occurrences\n",
      "ECON_MONEYLAUNDERING                               :   19 occurrences\n",
      "WB_2076_MONEY_LAUNDERING                           :   19 occurrences\n",
      "WB_969_CAPITAL_MARKETS_LAW_AND_REGULATION          :   19 occurrences\n",
      "WB_1152_ECONOMIC_AND_SOCIAL_MOBILITY               :   19 occurrences\n",
      "WB_2112_MARKET_DEFINITION_AND_MARKET_POWER         :   19 occurrences\n",
      "WB_1540_ECONOMICS_OF_EDUCATION                     :   18 occurrences\n",
      "ECON_HEATINGOIL                                    :   18 occurrences\n",
      "EPU_CATS_SOVEREIGN_DEBT_CURRENCY_CRISES            :   17 occurrences\n",
      "WB_2482_RECONCILIATION                             :   16 occurrences\n",
      "ECON_DEVELOPMENTORGS_WORLD_BANK                    :   16 occurrences\n",
      "ECON_WORLDCURRENCIES_YEN                           :   15 occurrences\n",
      "SOC_ECONCOOP                                       :   15 occurrences\n",
      "TAX_FNCACT_BUSINESS_LEADERS                        :   15 occurrences\n",
      "WB_2788_LABOR_MARKET_INSTITUTIONS_AND_POLICIES     :   14 occurrences\n",
      "ECON_PRICEMANIPULATION                             :   14 occurrences\n",
      "ECON_WORLDCURRENCIES_EURO                          :   14 occurrences\n",
      "ECON_GASOLINEPRICE                                 :   14 occurrences\n",
      "ECON_WORLDCURRENCIES_YUAN                          :   13 occurrences\n",
      "ECON_WORKINGCLASS                                  :   13 occurrences\n",
      "ECON_MOU                                           :   13 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS                       :   13 occurrences\n",
      "WB_862_GROWTH_POLES_AND_ECONOMIC_ZONES             :   12 occurrences\n",
      "ECON_MIDDLECLASS                                   :   12 occurrences\n",
      "ECON_BANKRUPTCY                                    :   11 occurrences\n",
      "ECON_FOREIGNINVEST                                 :   11 occurrences\n",
      "ECON_SOVEREIGN_DEBT                                :   11 occurrences\n",
      "WB_411_DEBT_RESOLUTION                             :   10 occurrences\n",
      "WB_1245_FINANCIAL_VULNERABILITY_AND_RISKS          :   10 occurrences\n",
      "WB_1844_MARKET_BASED_CLIMATE_CHANGE_MITIGATION     :   10 occurrences\n",
      "BLACK_MARKET                                       :   10 occurrences\n",
      "ECON_IDENTITYTHEFT                                 :    9 occurrences\n",
      "WB_2073_ILLICIT_FINANCIAL_FLOWS                    :    9 occurrences\n",
      "ECON_EMERGINGECON                                  :    8 occurrences\n",
      "ECON_DEFLATION                                     :    8 occurrences\n",
      "ECON_IPO                                           :    8 occurrences\n",
      "WB_1277_BANKRUPTCY_AND_LIQUIDATION                 :    8 occurrences\n",
      "WB_1236_COMMERCIAL_BANKING                         :    8 occurrences\n",
      "ECON_NATGASPRICE                                   :    8 occurrences\n",
      "TAX_FNCACT_TRADER                                  :    8 occurrences\n",
      "ECON_DEVELOPMENTORGS_WORLD_FOOD_PROGRAM            :    7 occurrences\n",
      "WB_2575_TRADE_POLICY_AND_INVESTMENT_AGREEMENTS     :    7 occurrences\n",
      "WB_718_PUBLIC_INVESTMENT_MANAGEMENT                :    7 occurrences\n",
      "ECON_PROPANE                                       :    7 occurrences\n",
      "WB_371_BRANCHLESS_BANKING                          :    7 occurrences\n",
      "WB_481_SECONDARY_EDUCATION                         :    7 occurrences\n",
      "ECON_DEVELOPMENTORGS_ORGANISATION_FOR_ECONOMIC_COOPERATION :    6 occurrences\n",
      "WB_1918_SECURITIES_MARKETS                         :    6 occurrences\n",
      "ECON_EARNINGSREPORT                                :    6 occurrences\n",
      "SLFID_ECONOMIC_POWER                               :    6 occurrences\n",
      "WB_2081_FINANCIAL_DISCLOSURE                       :    6 occurrences\n",
      "WB_780_TRADE_IN_SERVICES                           :    6 occurrences\n",
      "TAX_FNCACT_BUSINESSMEN                             :    6 occurrences\n",
      "WB_1254_INTERNET_BANKING                           :    6 occurrences\n",
      "ECON_WORLDCURRENCIES_PESO                          :    6 occurrences\n",
      "ECON_CURRENCY_RESERVES                             :    6 occurrences\n",
      "EPU_POLICY_BANK_OF_JAPAN                           :    5 occurrences\n",
      "ECON_MONOPOLY                                      :    5 occurrences\n",
      "WB_1068_MARKET_FAILURES_VERSUS_GOVERNMENT_FAILURES :    5 occurrences\n",
      "WB_440_MACROECONOMIC_MONITORING                    :    5 occurrences\n",
      "WB_334_EQUITY_MARKETS                              :    5 occurrences\n",
      "WB_351_PAYMENT_AND_MARKET_INFRASTRUCTURE           :    5 occurrences\n",
      "ECON_WORLDCURRENCIES_PESOS                         :    5 occurrences\n",
      "WB_1309_SECONDARY_CARE                             :    5 occurrences\n",
      "ECON_ELECTRICALPRICE                               :    5 occurrences\n",
      "ECON_REMITTANCE                                    :    5 occurrences\n",
      "WB_1285_BUSINESS_TAXATION                          :    5 occurrences\n",
      "ECON_FREETRADE                                     :    5 occurrences\n",
      "WB_452_DEBT_RELIEF                                 :    5 occurrences\n",
      "ECON_WORLDCURRENCIES_US_DOLLAR                     :    5 occurrences\n",
      "WB_2424_ICT_AND_FINANCIAL_SECTOR                   :    4 occurrences\n",
      "WB_827_MOBILE_MONEY                                :    4 occurrences\n",
      "ECON_WORLDCURRENCIES_US_DOLLARS                    :    4 occurrences\n",
      "ECON_WORLDCURRENCIES_JAPANESE_YEN                  :    4 occurrences\n",
      "TAX_FNCACT_BANKER                                  :    4 occurrences\n",
      "ECON_ELECTRICALDEMAND                              :    4 occurrences\n",
      "ECON_WORLDCURRENCIES_CANADIAN_DOLLAR               :    4 occurrences\n",
      "ECON_DEVELOPMENTORGS_WORLD_FOOD_PROGRAMME          :    4 occurrences\n",
      "WB_1747_PRODUCT_MARKET_REGULATION_AND_COMPETITION_ADVOCACY :    4 occurrences\n",
      "WB_2581_PREFERENTIAL_TRADE_AND_INVESTMENT_AGREEMENTS :    4 occurrences\n",
      "TAX_FNCACT_FINANCIAL_ADVISER                       :    4 occurrences\n",
      "ECON_TRANSPORT_COST                                :    4 occurrences\n",
      "TAX_FNCACT_FINANCIAL_PLANNER                       :    3 occurrences\n",
      "WB_2387_BROADCASTING_INDUSTRY_AND_MARKETS          :    3 occurrences\n",
      "WB_776_TRADE_POLICY                                :    3 occurrences\n",
      "ECON_PAY_CUTS                                      :    3 occurrences\n",
      "WB_1058_AGRIBUSINESS                               :    3 occurrences\n",
      "WB_2179_AGRIBUSINESS                               :    3 occurrences\n",
      "WB_331_ECONOMIC_TRANSPARENCY                       :    3 occurrences\n",
      "WB_865_TRADE_CORRIDORS                             :    3 occurrences\n",
      "CRISISLEX_T05_MONEY                                :    3 occurrences\n",
      "WB_2563_TRADE_AND_FDI_PROMOTION_INFRASTRUCTURE     :    3 occurrences\n",
      "ECON_WORLDCURRENCIES_FRANC                         :    3 occurrences\n",
      "ECON_WORLDCURRENCIES_FRANCS                        :    3 occurrences\n",
      "WB_1098_MONETARY_AND_FINANCIAL_STABILITY           :    3 occurrences\n",
      "WB_1096_MACROECONOMIC_SUSTAINABILITY               :    3 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_EUROPEAN_ECONOMIC_AREA :    3 occurrences\n",
      "TAX_FNCACT_PRIVATE_INVESTIGATOR                    :    3 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_TPP                   :    3 occurrences\n",
      "ECON_TRADE_DISPUTE                                 :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_CANADIAN_DOLLARS              :    2 occurrences\n",
      "ECON_DEVELOPMENTORGS_UNITED_NATIONS_ENVIRONMENT_PROGRAMME :    2 occurrences\n",
      "WB_1015_TRADE_REGULATION                           :    2 occurrences\n",
      "ECON_UNDEREMPLOYMENT                               :    2 occurrences\n",
      "EPU_POLICY_PEOPLE_BANK_OF_CHINA                    :    2 occurrences\n",
      "WB_1016_SPECIAL_ECONOMIC_ZONES                     :    2 occurrences\n",
      "ECON_DEREGULATION                                  :    2 occurrences\n",
      "WB_427_WATER_ALLOCATION_AND_WATER_ECONOMICS        :    2 occurrences\n",
      "WB_1959_LINKING_FARMERS_WITH_MARKETS               :    2 occurrences\n",
      "WB_668_BUSINESS_CONTINUITY                         :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_SWISS_FRANC                   :    2 occurrences\n",
      "TAX_FNCACT_BUSINESS_EXECUTIVE                      :    2 occurrences\n",
      "WB_2106_MARKET_COMPETITION_ADVOCACY_AND_LIBERALIZATION :    2 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_FREE_TRADE_AGREEMENT  :    2 occurrences\n",
      "ECON_INFORMAL_ECONOMY                              :    2 occurrences\n",
      "WB_1247_MARKET_RISKS                               :    2 occurrences\n",
      "TAX_FNCACT_BUSINESS_LEADER                         :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_BRITISH_POUND                 :    2 occurrences\n",
      "TAX_FNCACT_BUSINESSWOMEN                           :    2 occurrences\n",
      "ECON_DEVELOPMENTORGS_ORGANIZATION_FOR_ECONOMIC_COOPERATION :    2 occurrences\n",
      "ECON_DEVELOPMENTORGS_JAPAN_INTERNATIONAL_COOPERATION_AGENCY :    2 occurrences\n",
      "ECON_PRICECONTROL                                  :    2 occurrences\n",
      "AID_ECONOMIC                                       :    2 occurrences\n",
      "WB_836_POLITICAL_ECONOMY_OF_REFORM                 :    2 occurrences\n",
      "ECON_DEVELOPMENTORGS_WORLD_HEALTH_ORGANISATION     :    2 occurrences\n",
      "ECON_DEVELOPMENTORGS_GATES_FOUNDATION              :    2 occurrences\n",
      "WB_2608_TRADE_IN_GOODS                             :    2 occurrences\n",
      "WB_765_TRADE_COMPETITIVENESS_AND_DIVERSIFICATION   :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_SHILLING                      :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_BRITISH_POUNDS                :    2 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_FREE_TRADE_AGREEMENTS :    2 occurrences\n",
      "WB_454_DEBT_RESTRUCTURING                          :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_ROUBLES                       :    2 occurrences\n",
      "ECON_WORLDCURRENCIES_CHINESE_YUAN                  :    2 occurrences\n",
      "WB_769_INVESTMENT_POLICY                           :    2 occurrences\n",
      "WB_2552_INVESTMENT_PROMOTION                       :    2 occurrences\n",
      "TAX_FNCACT_TRADESMAN                               :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_INDIAN_RUPEE                  :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_GHANA_CEDIS                   :    1 occurrences\n",
      "WB_2532_DEBT_RECOVERY                              :    1 occurrences\n",
      "ECON_COUNTERFEITMONEY                              :    1 occurrences\n",
      "ECON_DEVELOPMENTORGS_UNITED_NATIONS_CHILDREN_FUND  :    1 occurrences\n",
      "ECON_GOLDPRICE                                     :    1 occurrences\n",
      "WB_312_INTERNATIONAL_FINANCIAL_STANDARDS           :    1 occurrences\n",
      "TAX_FNCACT_INVESTMENT_BANKER                       :    1 occurrences\n",
      "WB_1278_BUSINESS_ENTRY_AND_STARTUP                 :    1 occurrences\n",
      "ECON_DIESELPRICE                                   :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_DOMINICAN_PESOS               :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_THE_RUPEE                     :    1 occurrences\n",
      "EPU_POLICY_PUBLIC_INVESTMENT                       :    1 occurrences\n",
      "ECON_DEVELOPMENTORGS_AFRICAN_DEVELOPMENT_BANK      :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_SWISS_FRANCS                  :    1 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_NAFTA                 :    1 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_NORTH_AMERICAN_FREE_TRADE_AGREEMENT :    1 occurrences\n",
      "WB_451_DEBT_MANAGEMENT                             :    1 occurrences\n",
      "ECON_DEVELOPMENTORGS_INTERNATIONAL_ORGANIZATION_FOR_MIGRATION :    1 occurrences\n",
      "TAX_FNCACT_BUSINESS_EXECUTIVES                     :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_NEW_ZEALAND_DOLLAR            :    1 occurrences\n",
      "TAX_FNCACT_STOCK_CLERK                             :    1 occurrences\n",
      "ECON_DEVELOPMENTORGS_WORLD_BANK_GROUP              :    1 occurrences\n",
      "WB_2459_ILLICIT_FINANCIAL_FLOWS                    :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_ARGENTINE_PESO                :    1 occurrences\n",
      "WB_1748_DEBT_FUNDS                                 :    1 occurrences\n",
      "WB_2124_ECONOMIC_ANALYSIS_OF_EDUCATION             :    1 occurrences\n",
      "WB_370_TRADE_FINANCE                               :    1 occurrences\n",
      "ECON_DEVELOPMENTORGS_UNITED_NATIONS_CONFERENCE_ON_TRADE :    1 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_GULF_COOPERATION_COUNCIL :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_GHANA_CEDI                    :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_UGANDAN_SHILLING              :    1 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_EAST_AFRICAN_COMMUNITY :    1 occurrences\n",
      "TAX_FNCACT_STOCKBROKER                             :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_PAKISTANI_RUPEES              :    1 occurrences\n",
      "WB_2506_ARMS_TRADE                                 :    1 occurrences\n",
      "WB_317_BANK_REGULATION_AND_SUPERVISION             :    1 occurrences\n",
      "WB_1241_BANK_CAPITAL_ADEQUACY                      :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_NEW_TAIWAN_DOLLARS            :    1 occurrences\n",
      "TAX_ECON_FREETRADEAGREEMENTS_FREETRADE_AGREEMENT   :    1 occurrences\n",
      "WB_2548_INVESTMENT_PROTECTION                      :    1 occurrences\n",
      "WB_1284_INVESTMENT_INCENTIVES                      :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_ROUBLE                        :    1 occurrences\n",
      "WB_1237_MERCHANT_BANKS                             :    1 occurrences\n",
      "WB_2922_ACCESS_TO_SECONDARY_EDUCATION              :    1 occurrences\n",
      "ECON_BUBBLE                                        :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_EUROS                         :    1 occurrences\n",
      "WB_2861_JOBS_IN_TRADE_AND_COMPETITIVENESS          :    1 occurrences\n",
      "ECON_WORLDCURRENCIES_AUSTRALIAN_DOLLAR             :    1 occurrences\n",
      "WB_2113_MARKET_STUDIES_AND_COMPETITION_ASSESSMENTS :    1 occurrences\n",
      "WB_1252_MOBILE_BANKING                             :    1 occurrences\n",
      "\n",
      "Total unique financial themes found: 267\n",
      "Total financial theme mentions: 14917\n",
      "\n",
      "=== TOP 10 MOST COMMON THEMES OVERALL ===\n",
      "                                                   : 7300 occurrences\n",
      "TAX_FNCACT                                         : 6758 occurrences\n",
      "CRISISLEX_CRISISLEXREC                             : 3875 occurrences\n",
      "EPU_POLICY                                         : 3156 occurrences\n",
      "TAX_ETHNICITY                                      : 2808 occurrences\n",
      "UNGP_FORESTS_RIVERS_OCEANS                         : 2671 occurrences\n",
      "CRISISLEX_C07_SAFETY                               : 2584 occurrences\n",
      "SOC_POINTSOFINTEREST                               : 2454 occurrences\n",
      "WB_696_PUBLIC_SECTOR_MANAGEMENT                    : 2424 occurrences\n",
      "USPEC_POLITICS_GENERAL1                            : 2278 occurrences\n",
      "\n",
      "=== SENTIMENT ANALYSIS ===\n",
      "Sentiment score statistics (tone_avg):\n",
      "count    4786.000000\n",
      "mean       -1.892067\n",
      "std         3.554161\n",
      "min       -13.874346\n",
      "25%        -4.246649\n",
      "50%        -1.829170\n",
      "75%         0.434467\n",
      "max        14.705882\n",
      "Name: tone_avg, dtype: float64\n",
      "\n",
      "Sentiment breakdown:\n",
      "Negative events (tone < -2): 2302\n",
      "Neutral events (-2 <= tone <= 2): 1850\n",
      "Positive events (tone > 2): 634\n"
     ]
    }
   ],
   "source": [
    "# --- EXPLORE AVAILABLE THEMES ---\n",
    "# Let's see what financial/economic themes are actually in your data\n",
    "\n",
    "print(\"=== THEME ANALYSIS ===\")\n",
    "\n",
    "# Get all unique themes from the dataset\n",
    "all_themes = []\n",
    "for themes_str in df['Themes'].dropna():\n",
    "    themes_list = str(themes_str).split(';')\n",
    "    all_themes.extend(themes_list)\n",
    "\n",
    "# Count theme frequency\n",
    "from collections import Counter\n",
    "theme_counts = Counter(all_themes)\n",
    "\n",
    "# Find financial/economic related themes\n",
    "financial_keywords = ['ECON', 'FINANCIAL', 'BUSINESS', 'MARKET', 'MONEY', 'BANK', 'INVEST', 'TRADE', 'ECONOMY', 'GDP', 'INFLATION', 'DEBT', 'STOCK']\n",
    "\n",
    "print(\"Financial/Economic themes found in your data:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "financial_themes_found = {}\n",
    "for theme, count in theme_counts.most_common():\n",
    "    for keyword in financial_keywords:\n",
    "        if keyword in theme.upper():\n",
    "            financial_themes_found[theme] = count\n",
    "            break\n",
    "\n",
    "# Display financial themes sorted by frequency\n",
    "for theme, count in sorted(financial_themes_found.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{theme:<50} : {count:>4} occurrences\")\n",
    "\n",
    "print(f\"\\nTotal unique financial themes found: {len(financial_themes_found)}\")\n",
    "print(f\"Total financial theme mentions: {sum(financial_themes_found.values())}\")\n",
    "\n",
    "# Show top 10 most common themes overall (to understand the data better)\n",
    "print(f\"\\n=== TOP 10 MOST COMMON THEMES OVERALL ===\")\n",
    "for theme, count in theme_counts.most_common(10):\n",
    "    print(f\"{theme:<50} : {count:>4} occurrences\")\n",
    "\n",
    "# Show some sentiment statistics\n",
    "print(f\"\\n=== SENTIMENT ANALYSIS ===\")\n",
    "if 'tone_avg' in forecasting_dataset.columns:\n",
    "    sentiment_stats = forecasting_dataset['tone_avg'].describe()\n",
    "    print(\"Sentiment score statistics (tone_avg):\")\n",
    "    print(sentiment_stats)\n",
    "    \n",
    "    negative_events = len(forecasting_dataset[forecasting_dataset['tone_avg'] < -2])\n",
    "    positive_events = len(forecasting_dataset[forecasting_dataset['tone_avg'] > 2])\n",
    "    neutral_events = len(forecasting_dataset[abs(forecasting_dataset['tone_avg']) <= 2])\n",
    "    \n",
    "    print(f\"\\nSentiment breakdown:\")\n",
    "    print(f\"Negative events (tone < -2): {negative_events}\")\n",
    "    print(f\"Neutral events (-2 <= tone <= 2): {neutral_events}\")\n",
    "    print(f\"Positive events (tone > 2): {positive_events}\")\n",
    "else:\n",
    "    print(\"Sentiment parsing failed - check V2Tone column format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60ec83",
   "metadata": {},
   "source": [
    "## Ultra-Selective Financial Event Filtering ðŸŽ¯âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ULTRA-SELECTIVE FILTERING FOR FINANCIAL FORECASTING ---\n",
    "# Target: 100-300 events per year (about 2-6 events per week)\n",
    "# Strategy: Only the most market-moving events\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the existing filtered data\n",
    "df = pd.read_csv('data/gdelt_financial_filtered.csv')\n",
    "print(f\"Starting with {len(df)} events from previous filtering\")\n",
    "\n",
    "# --- TIER 1: HIGHEST IMPACT THEMES ONLY ---\n",
    "# These are the themes most likely to directly impact stock markets\n",
    "tier1_themes = [\n",
    "    'ECON_STOCKMARKET',                    # Direct stock market mentions\n",
    "    'ECON_INFLATION',                      # Inflation announcements\n",
    "    'ECON_RECESSION',                      # Recession indicators\n",
    "    'ECON_GDP',                           # GDP announcements\n",
    "    'ECON_UNEMPLOYMENT',                  # Employment data releases\n",
    "    'ECON_INTEREST_RATE',                 # Interest rate decisions\n",
    "    'ECON_CURRENCY',                      # Currency crises\n",
    "    'EPU_ECONOMY',                        # Major economic uncertainty\n",
    "    'WB_1920_FINANCIAL_SECTOR_DEVELOPMENT', # Financial sector events\n",
    "    'WB_439_MACROECONOMIC_AND_STRUCTURAL_POLICIES', # Major policy changes\n",
    "]\n",
    "\n",
    "# --- TIER 2: MAJOR FINANCIAL INSTITUTIONS & CENTRAL BANKS ---\n",
    "# Events from these sources are typically market-moving\n",
    "tier2_themes = [\n",
    "    'WB_.*MONETARY.*',                    # Monetary policy\n",
    "    'WB_.*CENTRAL_BANK.*',               # Central bank actions\n",
    "    'WB_.*FINANCIAL_CRISIS.*',           # Financial crisis events\n",
    "    'CRISISLEX.*FINANCIAL',              # Financial crisis lexicon\n",
    "    'EPU_POLICY',                        # Major policy announcements\n",
    "]\n",
    "\n",
    "# --- SENTIMENT-BASED FILTERING ---\n",
    "# Only events with extreme sentiment (very positive or very negative)\n",
    "def is_extreme_sentiment(tone_avg):\n",
    "    \"\"\"Check if sentiment is extreme enough to potentially move markets\"\"\"\n",
    "    try:\n",
    "        tone = float(tone_avg)\n",
    "        return abs(tone) >= 5.0  # Very strong positive or negative sentiment\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# --- LOCATION-BASED FILTERING ---\n",
    "# Only events in major financial centers\n",
    "tier1_locations = [\n",
    "    'New York', 'Manhattan', 'Wall Street',           # US Financial Center\n",
    "    'London', 'City of London',                       # UK Financial Center  \n",
    "    'Frankfurt', 'European Central Bank',             # EU Financial Center\n",
    "    'Tokyo', 'Nikkei',                               # Japan Financial Center\n",
    "    'Washington', 'Federal Reserve',                  # US Policy Center\n",
    "    'Brussels', 'European Union',                     # EU Policy Center\n",
    "]\n",
    "\n",
    "# --- MAJOR NEWS SOURCES ONLY ---\n",
    "# Focus on major financial news sources that markets actually follow\n",
    "major_financial_sources = [\n",
    "    'reuters', 'bloomberg', 'wsj', 'ft.com', 'cnbc', 'marketwatch', \n",
    "    'yahoo.com', 'cnn.com', 'bbc', 'ap.org', 'economist.com',\n",
    "    'financial-times', 'wall-street-journal', 'associated-press'\n",
    "]\n",
    "\n",
    "# Apply Tier 1 filtering (most restrictive)\n",
    "print(\"\\n=== APPLYING ULTRA-SELECTIVE FILTERS ===\")\n",
    "\n",
    "# Filter 1: Tier 1 themes only\n",
    "tier1_pattern = '|'.join(tier1_themes)\n",
    "tier1_filter = df['Themes'].str.contains(tier1_pattern, case=False, na=False, regex=True)\n",
    "tier1_events = df[tier1_filter].copy()\n",
    "print(f\"After Tier 1 themes: {len(tier1_events)} events\")\n",
    "\n",
    "# Filter 2: Add Tier 2 themes for broader coverage\n",
    "tier2_pattern = '|'.join(tier2_themes) \n",
    "tier2_filter = df['Themes'].str.contains(tier2_pattern, case=False, na=False, regex=True)\n",
    "tier1_and_2_events = df[tier1_filter | tier2_filter].copy()\n",
    "print(f\"After adding Tier 2 themes: {len(tier1_and_2_events)} events\")\n",
    "\n",
    "# Filter 3: Extreme sentiment only\n",
    "extreme_sentiment_filter = tier1_and_2_events['tone_avg'].apply(is_extreme_sentiment)\n",
    "high_impact_events = tier1_and_2_events[extreme_sentiment_filter].copy()\n",
    "print(f\"After extreme sentiment filtering: {len(high_impact_events)} events\")\n",
    "\n",
    "# Filter 4: Major financial centers only\n",
    "location_pattern = '|'.join(tier1_locations)\n",
    "location_filter = high_impact_events['Locations'].str.contains(location_pattern, case=False, na=False)\n",
    "geographic_filtered = high_impact_events[location_filter].copy()\n",
    "print(f\"After major financial centers: {len(geographic_filtered)} events\")\n",
    "\n",
    "# Filter 5: Major news sources only\n",
    "source_pattern = '|'.join(major_financial_sources)\n",
    "source_filter = geographic_filtered['SourceCommonName'].str.contains(source_pattern, case=False, na=False)\n",
    "source_filtered = geographic_filtered[source_filter].copy()\n",
    "print(f\"After major financial sources: {len(source_filtered)} events\")\n",
    "\n",
    "# --- FINAL SELECTION APPROACH ---\n",
    "# If still too many events, use additional criteria\n",
    "\n",
    "if len(source_filtered) > 50:  # Still too many for one week\n",
    "    print(f\"\\nStill {len(source_filtered)} events - applying final selection criteria:\")\n",
    "    \n",
    "    # Criteria 1: Word count (longer articles are typically more significant)\n",
    "    source_filtered['word_count_score'] = source_filtered['word_count'] / source_filtered['word_count'].max()\n",
    "    \n",
    "    # Criteria 2: Sentiment extremity (more extreme = more important)\n",
    "    source_filtered['sentiment_extremity'] = abs(source_filtered['tone_avg'])\n",
    "    source_filtered['sentiment_score'] = source_filtered['sentiment_extremity'] / source_filtered['sentiment_extremity'].max()\n",
    "    \n",
    "    # Criteria 3: Theme relevance (count of financial themes)\n",
    "    def count_financial_themes(themes_str):\n",
    "        if pd.isna(themes_str):\n",
    "            return 0\n",
    "        count = 0\n",
    "        for theme in tier1_themes + tier2_themes:\n",
    "            if theme.replace('.*', '') in str(themes_str):\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    source_filtered['theme_count'] = source_filtered['Themes'].apply(count_financial_themes)\n",
    "    source_filtered['theme_score'] = source_filtered['theme_count'] / max(source_filtered['theme_count'].max(), 1)\n",
    "    \n",
    "    # Combined importance score\n",
    "    source_filtered['importance_score'] = (\n",
    "        0.4 * source_filtered['sentiment_score'] +      # 40% sentiment\n",
    "        0.3 * source_filtered['theme_score'] +          # 30% theme relevance  \n",
    "        0.3 * source_filtered['word_count_score']       # 30% article length\n",
    "    )\n",
    "    \n",
    "    # Select top 20 most important events per week\n",
    "    final_events = source_filtered.nlargest(20, 'importance_score')\n",
    "    \n",
    "else:\n",
    "    final_events = source_filtered.copy()\n",
    "    final_events['importance_score'] = 1.0  # All events are important\n",
    "\n",
    "print(f\"\\n=== FINAL SELECTION ===\")\n",
    "print(f\"Ultra-filtered events: {len(final_events)} events\")\n",
    "print(f\"Projected yearly events: {len(final_events) * 52} events/year\")\n",
    "\n",
    "# --- SAVE ULTRA-FILTERED DATASET ---\n",
    "# Clean up and save\n",
    "final_columns = [\n",
    "    'DATE', 'date_only', 'SourceCommonName', 'DocumentIdentifier',\n",
    "    'tone_avg', 'tone_positive', 'tone_negative', 'tone_polarity',\n",
    "    'Themes', 'Locations', 'word_count', 'importance_score'\n",
    "]\n",
    "\n",
    "ultra_filtered_df = final_events[final_columns].copy()\n",
    "ultra_filtered_df.to_csv('data/gdelt_ultra_filtered.csv', index=False)\n",
    "\n",
    "print(f\"Saved ultra-filtered dataset to: data/gdelt_ultra_filtered.csv\")\n",
    "\n",
    "# --- SHOW SAMPLE RESULTS ---\n",
    "print(f\"\\n=== SAMPLE OF MOST IMPORTANT EVENTS ===\")\n",
    "if len(final_events) > 0:\n",
    "    top_events = final_events.nlargest(5, 'importance_score')\n",
    "    for idx, row in top_events.iterrows():\n",
    "        print(f\"\\nðŸ“ˆ Importance Score: {row['importance_score']:.3f}\")\n",
    "        print(f\"ðŸ“… Date: {row['date_only']}\")\n",
    "        print(f\"ðŸ“° Source: {row['SourceCommonName']}\")\n",
    "        print(f\"ðŸ˜Š Sentiment: {row['tone_avg']:.1f}\")\n",
    "        print(f\"ðŸ“ Word Count: {row['word_count']}\")\n",
    "        print(f\"ðŸ”— URL: {row['DocumentIdentifier'][:80]}...\")\n",
    "        print(f\"ðŸ·ï¸ Key Themes: {str(row['Themes'])[:100]}...\")\n",
    "else:\n",
    "    print(\"No events found with current criteria - consider relaxing filters\")\n",
    "\n",
    "print(f\"\\nâœ… ULTRA-FILTERING COMPLETE!\")\n",
    "print(f\"ðŸ“Š Target achieved: ~{len(final_events) * 52} events per year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6b60a",
   "metadata": {},
   "source": [
    "## USA-Focused Financial Event Filtering ðŸ‡ºðŸ‡¸ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67f48ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 4786 events from previous filtering\n",
      "After USA location filtering: 3614 events\n",
      "After USA financial themes: 3265 events\n",
      "After US financial sources: 351 events\n",
      "After extreme sentiment (Â±7.0): 23 events\n",
      "After market hours/business days: 19 events\n",
      "\n",
      "=== USA FINANCIAL EVENTS FINAL SELECTION ===\n",
      "Ultra-filtered USA events: 19 events\n",
      "Projected yearly USA events: 988 events/year\n",
      "Saved USA-focused dataset to: data/gdelt_usa_financial.csv\n",
      "\n",
      "=== TOP USA FINANCIAL EVENTS ===\n",
      "\n",
      "ðŸ‡ºðŸ‡¸ USA Importance Score: 1.000\n",
      "ðŸ“… Date: 2024-01-09\n",
      "ðŸ“° Source: yahoo.com\n",
      "ðŸ˜Š Sentiment: -12.0\n",
      "ðŸ“ Word Count: 305.0\n",
      "ðŸ”— URL: https://news.yahoo.com/suspect-rape-12-old-culver-230028573.html...\n",
      "ðŸ·ï¸ Key Themes: TRIAL;RAPE;SOC_POINTSOFINTEREST;SOC_POINTSOFINTEREST_PRISON;WB_2495_DETENTION_PRISON_AND_CORRECTIONS...\n",
      "ðŸ“ USA Locations: 3#Kern County, California, United States#US#USCA#35.2961#-118.668#2054176;3#Los Angeles County, Cali...\n",
      "\n",
      "ðŸ‡ºðŸ‡¸ USA Importance Score: 1.000\n",
      "ðŸ“… Date: 2024-01-09\n",
      "ðŸ“° Source: yahoo.com\n",
      "ðŸ˜Š Sentiment: 9.4\n",
      "ðŸ“ Word Count: 177.0\n",
      "ðŸ”— URL: https://news.yahoo.com/fairfax-county-public-schools-receives-212147770.html...\n",
      "ðŸ·ï¸ Key Themes: EDUCATION;SOC_POINTSOFINTEREST;SOC_POINTSOFINTEREST_SCHOOLS;SOC_POINTSOFINTEREST_SCHOOL;PUBLIC_TRANS...\n",
      "ðŸ“ USA Locations: 1#United States#US#US#39.828175#-98.5795#US;3#Fairfax County, Virginia, United States#US#USVA#38.850...\n",
      "\n",
      "ðŸ‡ºðŸ‡¸ USA Importance Score: 1.000\n",
      "ðŸ“… Date: 2024-01-09\n",
      "ðŸ“° Source: yahoo.com\n",
      "ðŸ˜Š Sentiment: -7.9\n",
      "ðŸ“ Word Count: 257.0\n",
      "ðŸ”— URL: https://news.yahoo.com/son-accused-killing-mother-evansville-223153224.html...\n",
      "ðŸ·ï¸ Key Themes: TAX_FNCACT;TAX_FNCACT_MAN;TRIAL;KILL;ARREST;SOC_POINTSOFINTEREST;SOC_POINTSOFINTEREST_JAIL;SOC_GENER...\n",
      "ðŸ“ USA Locations: 2#Indiana, United States#US#USIN#39.8647#-86.2604#IN;2#Illinois, United States#US#USIL#40.3363#-89.0...\n",
      "\n",
      "âœ… USA-FOCUSED FILTERING COMPLETE!\n",
      "ðŸŽ¯ Perfect for US market forecasting: ~988 events per year\n",
      "ðŸ“ˆ Focus: S&P 500, NASDAQ, US economic indicators\n"
     ]
    }
   ],
   "source": [
    "# --- USA-FOCUSED ULTRA-SELECTIVE FILTERING ---\n",
    "# Target: 50-150 events per year for USA financial markets\n",
    "# Strategy: Only US financial events that could impact S&P 500, NASDAQ, etc.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the existing filtered data\n",
    "df = pd.read_csv('data/gdelt_financial_filtered.csv')\n",
    "print(f\"Starting with {len(df)} events from previous filtering\")\n",
    "\n",
    "# --- USA-SPECIFIC FILTERING ---\n",
    "# Focus only on events within the United States\n",
    "usa_locations = [\n",
    "    'United States', 'New York', 'Manhattan', 'Wall Street', 'NYSE', 'NASDAQ',\n",
    "    'Washington', 'Federal Reserve', 'Fed', 'Treasury', 'SEC', \n",
    "    'California', 'Silicon Valley', 'San Francisco', 'Los Angeles',\n",
    "    'Chicago', 'Illinois', 'Boston', 'Massachusetts', 'Texas', 'Florida',\n",
    "    'White House', 'Congress', 'Senate', 'House of Representatives'\n",
    "]\n",
    "\n",
    "# Create USA location filter\n",
    "usa_pattern = '|'.join(usa_locations)\n",
    "usa_filter = df['Locations'].str.contains(usa_pattern, case=False, na=False)\n",
    "usa_events = df[usa_filter].copy()\n",
    "\n",
    "print(f\"After USA location filtering: {len(usa_events)} events\")\n",
    "\n",
    "# --- USA FINANCIAL THEMES (MOST CRITICAL) ---\n",
    "# Focus on themes that directly impact US markets\n",
    "usa_financial_themes = [\n",
    "    'ECON_STOCKMARKET',                    # Stock market events\n",
    "    'ECON_INFLATION',                      # US inflation data\n",
    "    'ECON_RECESSION',                      # US recession indicators\n",
    "    'ECON_GDP',                           # US GDP announcements\n",
    "    'ECON_UNEMPLOYMENT',                  # US employment data\n",
    "    'ECON_INTEREST_RATE',                 # Fed interest rates\n",
    "    'EPU_ECONOMY',                        # US economic uncertainty\n",
    "    'EPU_POLICY',                         # US policy changes\n",
    "    'WB_1920_FINANCIAL_SECTOR_DEVELOPMENT', # Financial sector\n",
    "    'WB_439_MACROECONOMIC_AND_STRUCTURAL_POLICIES', # Major US policies\n",
    "]\n",
    "\n",
    "# Apply theme filtering\n",
    "usa_theme_pattern = '|'.join(usa_financial_themes)\n",
    "usa_theme_filter = usa_events['Themes'].str.contains(usa_theme_pattern, case=False, na=False, regex=True)\n",
    "usa_themed_events = usa_events[usa_theme_filter].copy()\n",
    "\n",
    "print(f\"After USA financial themes: {len(usa_themed_events)} events\")\n",
    "\n",
    "# --- US FINANCIAL NEWS SOURCES ---\n",
    "# Focus on major US financial news sources\n",
    "us_financial_sources = [\n",
    "    'reuters.com', 'bloomberg', 'wsj', 'cnbc', 'marketwatch', 'yahoo.com',\n",
    "    'cnn.com', 'foxnews.com', 'abc', 'nbc', 'cbs', 'ap.org', 'usatoday',\n",
    "    'washingtonpost', 'nytimes', 'fortune', 'forbes', 'business-insider',\n",
    "    'thestreet', 'seeking-alpha', 'barrons'\n",
    "]\n",
    "\n",
    "# Apply source filtering\n",
    "us_source_pattern = '|'.join(us_financial_sources)\n",
    "source_filter = usa_themed_events['SourceCommonName'].str.contains(us_source_pattern, case=False, na=False)\n",
    "usa_source_events = usa_themed_events[source_filter].copy()\n",
    "\n",
    "print(f\"After US financial sources: {len(usa_source_events)} events\")\n",
    "\n",
    "# --- EXTREME SENTIMENT FOR US MARKETS ---\n",
    "# Only events with very strong sentiment (market-moving potential)\n",
    "def is_usa_market_moving_sentiment(tone_avg):\n",
    "    \"\"\"Check if sentiment is extreme enough to move US markets\"\"\"\n",
    "    try:\n",
    "        tone = float(tone_avg)\n",
    "        return abs(tone) >= 7.0  # Very extreme sentiment for US markets\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "extreme_usa_sentiment = usa_source_events['tone_avg'].apply(is_usa_market_moving_sentiment)\n",
    "usa_extreme_events = usa_source_events[extreme_usa_sentiment].copy()\n",
    "\n",
    "print(f\"After extreme sentiment (Â±7.0): {len(usa_extreme_events)} events\")\n",
    "\n",
    "# --- US MARKET HOURS & BUSINESS DAYS ---\n",
    "# Focus on events that happen during or close to US market hours\n",
    "usa_extreme_events['hour'] = pd.to_datetime(usa_extreme_events['DATE'], format='%Y%m%d%H%M%S').dt.hour\n",
    "usa_extreme_events['weekday'] = pd.to_datetime(usa_extreme_events['DATE'], format='%Y%m%d%H%M%S').dt.weekday\n",
    "\n",
    "# US market hours: 9:30 AM - 4:00 PM ET (roughly 14:30 - 21:00 UTC)\n",
    "# Business days: Monday-Friday (0-4 in Python weekday)\n",
    "market_hours_filter = (\n",
    "    (usa_extreme_events['hour'].between(6, 23)) &  # Extended hours for global impact\n",
    "    (usa_extreme_events['weekday'] < 5)  # Monday to Friday\n",
    ")\n",
    "\n",
    "usa_market_relevant = usa_extreme_events[market_hours_filter].copy()\n",
    "print(f\"After market hours/business days: {len(usa_market_relevant)} events\")\n",
    "\n",
    "# --- IMPORTANCE SCORING FOR USA EVENTS ---\n",
    "if len(usa_market_relevant) > 20:  # Still need further filtering\n",
    "    print(f\"\\nApplying final USA-specific importance scoring...\")\n",
    "    \n",
    "    # USA-specific scoring criteria\n",
    "    usa_market_relevant['usa_sentiment_score'] = abs(usa_market_relevant['tone_avg']) / 15.0  # Normalize to max 15\n",
    "    usa_market_relevant['usa_word_score'] = usa_market_relevant['word_count'] / usa_market_relevant['word_count'].max()\n",
    "    \n",
    "    # Count USA-specific financial themes\n",
    "    def count_usa_themes(themes_str):\n",
    "        if pd.isna(themes_str):\n",
    "            return 0\n",
    "        count = 0\n",
    "        for theme in usa_financial_themes:\n",
    "            if theme in str(themes_str):\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    usa_market_relevant['usa_theme_count'] = usa_market_relevant['Themes'].apply(count_usa_themes)\n",
    "    usa_market_relevant['usa_theme_score'] = usa_market_relevant['usa_theme_count'] / max(usa_market_relevant['usa_theme_count'].max(), 1)\n",
    "    \n",
    "    # USA market importance score\n",
    "    usa_market_relevant['usa_importance_score'] = (\n",
    "        0.5 * usa_market_relevant['usa_sentiment_score'] +      # 50% sentiment (most important for markets)\n",
    "        0.3 * usa_market_relevant['usa_theme_score'] +          # 30% theme relevance\n",
    "        0.2 * usa_market_relevant['usa_word_score']             # 20% article significance\n",
    "    )\n",
    "    \n",
    "    # Select top 10 most important USA events per week (targeting ~500/year)\n",
    "    final_usa_events = usa_market_relevant.nlargest(10, 'usa_importance_score')\n",
    "    \n",
    "else:\n",
    "    final_usa_events = usa_market_relevant.copy()\n",
    "    final_usa_events['usa_importance_score'] = 1.0\n",
    "\n",
    "print(f\"\\n=== USA FINANCIAL EVENTS FINAL SELECTION ===\")\n",
    "print(f\"Ultra-filtered USA events: {len(final_usa_events)} events\")\n",
    "print(f\"Projected yearly USA events: {len(final_usa_events) * 52} events/year\")\n",
    "\n",
    "# --- SAVE USA-FOCUSED DATASET ---\n",
    "usa_columns = [\n",
    "    'DATE', 'date_only', 'SourceCommonName', 'DocumentIdentifier',\n",
    "    'tone_avg', 'tone_positive', 'tone_negative', 'tone_polarity',\n",
    "    'Themes', 'Locations', 'word_count', 'usa_importance_score'\n",
    "]\n",
    "\n",
    "available_usa_columns = [col for col in usa_columns if col in final_usa_events.columns]\n",
    "usa_final_df = final_usa_events[available_usa_columns].copy()\n",
    "usa_final_df.to_csv('data/gdelt_usa_financial.csv', index=False)\n",
    "\n",
    "print(f\"Saved USA-focused dataset to: data/gdelt_usa_financial.csv\")\n",
    "\n",
    "# --- SHOW USA SAMPLE RESULTS ---\n",
    "print(f\"\\n=== TOP USA FINANCIAL EVENTS ===\")\n",
    "if len(final_usa_events) > 0:\n",
    "    for idx, row in final_usa_events.head(3).iterrows():\n",
    "        print(f\"\\nðŸ‡ºðŸ‡¸ USA Importance Score: {row.get('usa_importance_score', 'N/A'):.3f}\")\n",
    "        print(f\"ðŸ“… Date: {row['date_only']}\")\n",
    "        print(f\"ðŸ“° Source: {row['SourceCommonName']}\")\n",
    "        print(f\"ðŸ˜Š Sentiment: {row['tone_avg']:.1f}\")\n",
    "        print(f\"ðŸ“ Word Count: {row['word_count']}\")\n",
    "        print(f\"ðŸ”— URL: {row['DocumentIdentifier'][:80]}...\")\n",
    "        print(f\"ðŸ·ï¸ Key Themes: {str(row['Themes'])[:100]}...\")\n",
    "        print(f\"ðŸ“ USA Locations: {str(row['Locations'])[:100]}...\")\n",
    "else:\n",
    "    print(\"No USA events found - consider relaxing filters\")\n",
    "\n",
    "print(f\"\\nâœ… USA-FOCUSED FILTERING COMPLETE!\")\n",
    "print(f\"ðŸŽ¯ Perfect for US market forecasting: ~{len(final_usa_events) * 52} events per year\")\n",
    "print(f\"ðŸ“ˆ Focus: S&P 500, NASDAQ, US economic indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41569f89",
   "metadata": {},
   "source": [
    "## 10-Year USA Financial Data Collection ðŸ“…ðŸ‡ºðŸ‡¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‡ºðŸ‡¸ Starting 10-year USA financial data collection...\n",
      "ðŸ“… Period: 2015-01-01 to 2025-07-01\n",
      "â±ï¸ This will take several hours - progress will be saved periodically\n",
      "ðŸ“Š Total periods to collect: 127 months\n",
      "\n",
      "ðŸ“… Period 1/127: 2015-01-01 to 2015-01-31\n",
      "  âš ï¸ Error collecting data for this period: GDELT 2.0 only supports 'Feb 18 2015 - Present'queries currently. Try another date.\n",
      "\n",
      "ðŸ“… Period 2/127: 2015-02-01 to 2015-02-28\n",
      "  âš ï¸ Error collecting data for this period: GDELT 2.0 only supports 'Feb 18 2015 - Present'queries currently. Try another date.\n",
      "\n",
      "ðŸ“… Period 3/127: 2015-03-01 to 2015-03-31\n"
     ]
    }
   ],
   "source": [
    "# --- 10-YEAR USA FINANCIAL DATA COLLECTION ---\n",
    "# Systematic collection of GDELT data from 2015-2025 for USA financial events\n",
    "# Strategy: Collect weekly, apply USA filtering immediately to manage data volume\n",
    "\n",
    "import gdelt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "gd = gdelt.gdelt(version=2)\n",
    "\n",
    "# 10-year date range for your research\n",
    "start_date = \"2015-03-01\"\n",
    "end_date = \"2025-07-01\"  # 10 years of data\n",
    "\n",
    "# Output paths\n",
    "output_folder = \"data\"\n",
    "raw_output = os.path.join(output_folder, \"gdelt_usa_10year_raw.csv\")\n",
    "final_output = os.path.join(output_folder, \"gdelt_usa_10year_filtered.csv\")\n",
    "\n",
    "# --- USA FILTERING CRITERIA (Pre-defined for efficiency) ---\n",
    "usa_locations = [\n",
    "    'United States', 'New York', 'Manhattan', 'Wall Street', 'NYSE', 'NASDAQ',\n",
    "    'Washington', 'Federal Reserve', 'Fed', 'Treasury', 'SEC', \n",
    "    'California', 'Silicon Valley', 'San Francisco', 'Los Angeles',\n",
    "    'Chicago', 'Illinois', 'Boston', 'Massachusetts', 'Texas', 'Florida',\n",
    "    'White House', 'Congress', 'Senate', 'House of Representatives'\n",
    "]\n",
    "\n",
    "usa_financial_themes = [\n",
    "    'ECON_STOCKMARKET', 'ECON_INFLATION', 'ECON_RECESSION', 'ECON_GDP',\n",
    "    'ECON_UNEMPLOYMENT', 'ECON_INTEREST_RATE', 'EPU_ECONOMY', 'EPU_POLICY',\n",
    "    'WB_1920_FINANCIAL_SECTOR_DEVELOPMENT', 'WB_439_MACROECONOMIC_AND_STRUCTURAL_POLICIES'\n",
    "]\n",
    "\n",
    "us_financial_sources = [\n",
    "    'reuters', 'bloomberg', 'wsj', 'cnbc', 'marketwatch', 'yahoo',\n",
    "    'cnn', 'foxnews', 'abc', 'nbc', 'cbs', 'ap.org', 'usatoday',\n",
    "    'washingtonpost', 'nytimes', 'fortune', 'forbes'\n",
    "]\n",
    "\n",
    "# Create filter patterns\n",
    "usa_location_pattern = '|'.join(usa_locations)\n",
    "usa_theme_pattern = '|'.join(usa_financial_themes)\n",
    "usa_source_pattern = '|'.join(us_financial_sources)\n",
    "\n",
    "print(f\"ðŸ‡ºðŸ‡¸ Starting 10-year USA financial data collection...\")\n",
    "print(f\"ðŸ“… Period: {start_date} to {end_date}\")\n",
    "print(f\"â±ï¸ This will take several hours - progress will be saved periodically\")\n",
    "\n",
    "# --- Generate Monthly Date Ranges ---\n",
    "# Using monthly periods to balance API limits with reasonable collection speed\n",
    "monthly_ranges = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "\n",
    "print(f\"ðŸ“Š Total periods to collect: {len(monthly_ranges)} months\")\n",
    "\n",
    "# --- Collection with Real-time Filtering ---\n",
    "all_usa_events = []\n",
    "collection_stats = {\n",
    "    'total_periods': len(monthly_ranges),\n",
    "    'completed_periods': 0,\n",
    "    'total_raw_events': 0,\n",
    "    'total_usa_events': 0,\n",
    "    'failed_periods': 0\n",
    "}\n",
    "\n",
    "# Resume capability - check if we have partial data\n",
    "resume_from = 0\n",
    "if os.path.exists(raw_output):\n",
    "    print(\"ðŸ“‚ Found existing data file - checking for resume point...\")\n",
    "    try:\n",
    "        existing_df = pd.read_csv(raw_output)\n",
    "        if not existing_df.empty:\n",
    "            last_date = existing_df['DATE'].max()\n",
    "            last_date_parsed = pd.to_datetime(str(last_date), format='%Y%m%d%H%M%S')\n",
    "            # Find where to resume\n",
    "            for i, period_start in enumerate(monthly_ranges):\n",
    "                if period_start > last_date_parsed:\n",
    "                    resume_from = i\n",
    "                    break\n",
    "            print(f\"ðŸ”„ Resuming from period {resume_from + 1}/{len(monthly_ranges)}\")\n",
    "            all_usa_events.append(existing_df)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not resume from existing file: {e}\")\n",
    "        resume_from = 0\n",
    "\n",
    "# --- Main Collection Loop ---\n",
    "for i, month_start in enumerate(monthly_ranges[resume_from:], start=resume_from):\n",
    "    # Calculate period end (end of month)\n",
    "    if month_start.month == 12:\n",
    "        month_end = month_start.replace(year=month_start.year + 1, month=1, day=1) - timedelta(days=1)\n",
    "    else:\n",
    "        month_end = month_start.replace(month=month_start.month + 1, day=1) - timedelta(days=1)\n",
    "    \n",
    "    month_start_str = month_start.strftime('%Y-%m-%d')\n",
    "    month_end_str = month_end.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"\\nðŸ“… Period {i+1}/{len(monthly_ranges)}: {month_start_str} to {month_end_str}\")\n",
    "    \n",
    "    try:\n",
    "        # Search GDELT for the current month\n",
    "        results_df = gd.Search(\n",
    "            date=[month_start_str, month_end_str],\n",
    "            table='gkg',\n",
    "            output='df'\n",
    "        )\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            raw_count = len(results_df)\n",
    "            collection_stats['total_raw_events'] += raw_count\n",
    "            \n",
    "            # Apply USA location filter first (most restrictive)\n",
    "            usa_location_filter = results_df['Locations'].str.contains(\n",
    "                usa_location_pattern, case=False, na=False\n",
    "            )\n",
    "            usa_located = results_df[usa_location_filter].copy()\n",
    "            \n",
    "            if not usa_located.empty:\n",
    "                # Apply USA financial theme filter\n",
    "                usa_theme_filter = usa_located['Themes'].str.contains(\n",
    "                    usa_theme_pattern, case=False, na=False, regex=True\n",
    "                )\n",
    "                usa_themed = usa_located[usa_theme_filter].copy()\n",
    "                \n",
    "                if not usa_themed.empty:\n",
    "                    # Apply USA source filter\n",
    "                    usa_source_filter = usa_themed['SourceCommonName'].str.contains(\n",
    "                        usa_source_pattern, case=False, na=False\n",
    "                    )\n",
    "                    usa_final = usa_themed[usa_source_filter].copy()\n",
    "                    \n",
    "                    if not usa_final.empty:\n",
    "                        usa_count = len(usa_final)\n",
    "                        collection_stats['total_usa_events'] += usa_count\n",
    "                        all_usa_events.append(usa_final)\n",
    "                        \n",
    "                        print(f\"  âœ… Raw: {raw_count} â†’ USA Financial: {usa_count} events\")\n",
    "                    else:\n",
    "                        print(f\"  ðŸ“° No events after USA source filtering\")\n",
    "                else:\n",
    "                    print(f\"  ðŸ·ï¸ No events after USA theme filtering\")\n",
    "            else:\n",
    "                print(f\"  ðŸ“ No USA located events\")\n",
    "        else:\n",
    "            print(f\"  âŒ No events found for this period\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Error collecting data for this period: {e}\")\n",
    "        collection_stats['failed_periods'] += 1\n",
    "        continue\n",
    "    \n",
    "    collection_stats['completed_periods'] += 1\n",
    "    \n",
    "    # Save progress every 12 months\n",
    "    if (i + 1) % 12 == 0 and all_usa_events:\n",
    "        print(f\"\\nðŸ’¾ Saving progress at {month_start.year}...\")\n",
    "        temp_df = pd.concat(all_usa_events, ignore_index=True)\n",
    "        temp_df.to_csv(raw_output, index=False)\n",
    "        print(f\"   Saved {len(temp_df)} total USA events so far\")\n",
    "    \n",
    "    # Be respectful to GDELT servers\n",
    "    time.sleep(3)  # 3 seconds between requests for monthly data\n",
    "    \n",
    "    # Progress update every 6 months\n",
    "    if (i + 1) % 6 == 0:\n",
    "        print(f\"\\nðŸ“Š Progress Update:\")\n",
    "        print(f\"   Completed: {collection_stats['completed_periods']}/{collection_stats['total_periods']} periods\")\n",
    "        print(f\"   USA Events Collected: {collection_stats['total_usa_events']}\")\n",
    "        print(f\"   Failed Periods: {collection_stats['failed_periods']}\")\n",
    "\n",
    "# --- Final Processing ---\n",
    "print(f\"\\nðŸ Collection Complete!\")\n",
    "print(f\"ðŸ“ˆ Final Statistics:\")\n",
    "for key, value in collection_stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "if all_usa_events:\n",
    "    # Combine all collected data\n",
    "    final_usa_df = pd.concat(all_usa_events, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates (in case of overlapping periods)\n",
    "    final_usa_df = final_usa_df.drop_duplicates(subset=['DATE', 'DocumentIdentifier'])\n",
    "    \n",
    "    # Parse dates and add time features\n",
    "    final_usa_df['date_parsed'] = pd.to_datetime(final_usa_df['DATE'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "    final_usa_df['date_only'] = final_usa_df['date_parsed'].dt.date\n",
    "    final_usa_df['year'] = final_usa_df['date_parsed'].dt.year\n",
    "    final_usa_df['month'] = final_usa_df['date_parsed'].dt.month\n",
    "    final_usa_df['weekday'] = final_usa_df['date_parsed'].dt.weekday\n",
    "    \n",
    "    # Parse sentiment scores\n",
    "    def parse_v2tone_quick(v2tone_str):\n",
    "        try:\n",
    "            parts = str(v2tone_str).split(',')\n",
    "            if len(parts) >= 7:\n",
    "                return {\n",
    "                    'tone_avg': float(parts[0]),\n",
    "                    'tone_positive': float(parts[1]),\n",
    "                    'tone_negative': float(parts[2]),\n",
    "                    'word_count': int(parts[6])\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "        return {'tone_avg': 0, 'tone_positive': 0, 'tone_negative': 0, 'word_count': 0}\n",
    "    \n",
    "    sentiment_data = final_usa_df['V2Tone'].apply(parse_v2tone_quick)\n",
    "    sentiment_df = pd.json_normalize(sentiment_data)\n",
    "    \n",
    "    for col in sentiment_df.columns:\n",
    "        final_usa_df[col] = sentiment_df[col].values\n",
    "    \n",
    "    # Save raw collected data\n",
    "    final_usa_df.to_csv(raw_output, index=False)\n",
    "    print(f\"\\nðŸ’¾ Saved raw 10-year data: {raw_output}\")\n",
    "    print(f\"ðŸ“Š Total events collected: {len(final_usa_df)}\")\n",
    "    \n",
    "    # Show yearly breakdown\n",
    "    yearly_counts = final_usa_df.groupby('year').size()\n",
    "    print(f\"\\nðŸ“… Yearly Distribution:\")\n",
    "    for year, count in yearly_counts.items():\n",
    "        print(f\"   {year}: {count} events\")\n",
    "    \n",
    "    # Show average events per year\n",
    "    avg_per_year = len(final_usa_df) / len(yearly_counts)\n",
    "    print(f\"\\nðŸ“ˆ Average events per year: {avg_per_year:.0f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… 10-YEAR USA FINANCIAL DATA COLLECTION COMPLETE!\")\n",
    "    print(f\"ðŸŽ¯ Ready for advanced filtering and forecasting model development\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâŒ No data collected - check your filters and API connectivity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
