{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee8d52b",
   "metadata": {},
   "source": [
    "# ğŸ‡ºğŸ‡¸ 10-Year USA Financial Data Collection - Google Colab Edition\n",
    "\n",
    "**Purpose**: Collect 10 years of USA financial events from GDELT for forecasting research\n",
    "\n",
    "**Features**:\n",
    "- âœ… Automatic Google Drive integration\n",
    "- âœ… Resume capability if interrupted\n",
    "- âœ… Real-time progress tracking\n",
    "- âœ… Runs independently of your laptop\n",
    "- âœ… USA-focused financial filtering\n",
    "\n",
    "**Estimated Runtime**: 4-6 hours\n",
    "**Expected Output**: ~5,000 high-quality USA financial events (2015-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954976b",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gdelt pandas numpy\n",
    "\n",
    "# Import libraries\n",
    "import gdelt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from google.colab import drive, files\n",
    "import json\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe6c36",
   "metadata": {},
   "source": [
    "## ğŸ“ Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2628b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project folder in Google Drive\n",
    "project_folder = '/content/drive/MyDrive/Financial_Forecasting_Data'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Set up file paths\n",
    "raw_data_file = os.path.join(project_folder, 'gdelt_usa_10year_raw.csv')\n",
    "progress_file = os.path.join(project_folder, 'collection_progress.json')\n",
    "log_file = os.path.join(project_folder, 'collection_log.txt')\n",
    "\n",
    "print(f\"ğŸ“ Project folder created: {project_folder}\")\n",
    "print(f\"ğŸ“„ Raw data will be saved to: {raw_data_file}\")\n",
    "print(f\"ğŸ“Š Progress tracking: {progress_file}\")\n",
    "print(f\"ğŸ“ Logs will be saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172d528",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration & Filtering Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GDELT\n",
    "gd = gdelt.gdelt(version=2)\n",
    "\n",
    "# 10-year date range\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2025-01-01\"\n",
    "\n",
    "# USA-specific filtering criteria\n",
    "USA_LOCATIONS = [\n",
    "    'United States', 'New York', 'Manhattan', 'Wall Street', 'NYSE', 'NASDAQ',\n",
    "    'Washington', 'Federal Reserve', 'Fed', 'Treasury', 'SEC', \n",
    "    'California', 'Silicon Valley', 'San Francisco', 'Los Angeles',\n",
    "    'Chicago', 'Illinois', 'Boston', 'Massachusetts', 'Texas', 'Florida',\n",
    "    'White House', 'Congress', 'Senate', 'House of Representatives'\n",
    "]\n",
    "\n",
    "USA_FINANCIAL_THEMES = [\n",
    "    'ECON_STOCKMARKET', 'ECON_INFLATION', 'ECON_RECESSION', 'ECON_GDP',\n",
    "    'ECON_UNEMPLOYMENT', 'ECON_INTEREST_RATE', 'EPU_ECONOMY', 'EPU_POLICY',\n",
    "    'WB_1920_FINANCIAL_SECTOR_DEVELOPMENT', 'WB_439_MACROECONOMIC_AND_STRUCTURAL_POLICIES'\n",
    "]\n",
    "\n",
    "US_FINANCIAL_SOURCES = [\n",
    "    'reuters', 'bloomberg', 'wsj', 'cnbc', 'marketwatch', 'yahoo',\n",
    "    'cnn', 'foxnews', 'abc', 'nbc', 'cbs', 'ap.org', 'usatoday',\n",
    "    'washingtonpost', 'nytimes', 'fortune', 'forbes'\n",
    "]\n",
    "\n",
    "# Create filter patterns\n",
    "usa_location_pattern = '|'.join(USA_LOCATIONS)\n",
    "usa_theme_pattern = '|'.join(USA_FINANCIAL_THEMES)\n",
    "usa_source_pattern = '|'.join(US_FINANCIAL_SOURCES)\n",
    "\n",
    "print(f\"ğŸ‡ºğŸ‡¸ USA Location Filter: {len(USA_LOCATIONS)} keywords\")\n",
    "print(f\"ğŸ·ï¸ Financial Theme Filter: {len(USA_FINANCIAL_THEMES)} themes\")\n",
    "print(f\"ğŸ“° News Source Filter: {len(US_FINANCIAL_SOURCES)} sources\")\n",
    "print(f\"ğŸ“… Date Range: {START_DATE} to {END_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38291c3",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(message):\n",
    "    \"\"\"Log message to both console and file\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_entry = f\"[{timestamp}] {message}\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # Save to log file\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry + '\\n')\n",
    "\n",
    "def save_progress(stats, completed_periods):\n",
    "    \"\"\"Save progress to JSON file\"\"\"\n",
    "    progress_data = {\n",
    "        'last_updated': datetime.now().isoformat(),\n",
    "        'completed_periods': completed_periods,\n",
    "        'stats': stats\n",
    "    }\n",
    "    \n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress_data, f, indent=2)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load progress from JSON file\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        try:\n",
    "            with open(progress_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def parse_v2tone_quick(v2tone_str):\n",
    "    \"\"\"Quick V2Tone parsing for sentiment scores\"\"\"\n",
    "    try:\n",
    "        parts = str(v2tone_str).split(',')\n",
    "        if len(parts) >= 7:\n",
    "            return {\n",
    "                'tone_avg': float(parts[0]),\n",
    "                'tone_positive': float(parts[1]),\n",
    "                'tone_negative': float(parts[2]),\n",
    "                'word_count': int(parts[6])\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "    return {'tone_avg': 0, 'tone_positive': 0, 'tone_negative': 0, 'word_count': 0}\n",
    "\n",
    "def keep_alive():\n",
    "    \"\"\"Prevent Colab from disconnecting during long runs\"\"\"\n",
    "    from IPython.display import Javascript\n",
    "    display(Javascript('''\n",
    "        function ClickConnect(){\n",
    "            console.log(\"Working\");\n",
    "            document.querySelector(\"colab-toolbar-button#connect\").click();\n",
    "        }\n",
    "        setInterval(ClickConnect,60000);\n",
    "    '''))\n",
    "\n",
    "print(\"âœ… Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc15dcd",
   "metadata": {},
   "source": [
    "## ğŸ”„ Resume Capability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing data and resume point\n",
    "monthly_ranges = pd.date_range(start=START_DATE, end=END_DATE, freq='MS')\n",
    "resume_from = 0\n",
    "existing_data = []\n",
    "\n",
    "# Load previous progress\n",
    "progress_data = load_progress()\n",
    "if progress_data:\n",
    "    log_message(f\"ğŸ“‚ Found previous progress from {progress_data['last_updated']}\")\n",
    "    resume_from = progress_data.get('completed_periods', 0)\n",
    "    log_message(f\"ğŸ”„ Will resume from period {resume_from + 1}/{len(monthly_ranges)}\")\n",
    "\n",
    "# Check for existing raw data\n",
    "if os.path.exists(raw_data_file):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(raw_data_file)\n",
    "        if not existing_df.empty:\n",
    "            existing_data.append(existing_df)\n",
    "            log_message(f\"ğŸ“„ Found existing data with {len(existing_df)} events\")\n",
    "        else:\n",
    "            log_message(\"ğŸ“„ Found empty data file\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"âš ï¸ Could not read existing data file: {e}\")\n",
    "else:\n",
    "    log_message(\"ğŸ“„ No existing data file found - starting fresh\")\n",
    "\n",
    "# Initialize collection statistics\n",
    "collection_stats = {\n",
    "    'total_periods': len(monthly_ranges),\n",
    "    'completed_periods': resume_from,\n",
    "    'total_raw_events': 0,\n",
    "    'total_usa_events': len(existing_data[0]) if existing_data else 0,\n",
    "    'failed_periods': 0,\n",
    "    'start_time': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "log_message(f\"ğŸ“Š Collection will process {len(monthly_ranges) - resume_from} remaining periods\")\n",
    "log_message(f\"â±ï¸ Estimated remaining time: {(len(monthly_ranges) - resume_from) * 5 / 60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d782e",
   "metadata": {},
   "source": [
    "## ğŸ”Œ Keep Colab Alive (Run this to prevent disconnection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Colab session alive during long runs\n",
    "keep_alive()\n",
    "log_message(\"ğŸ”Œ Keep-alive activated - Colab will stay connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a4846",
   "metadata": {},
   "source": [
    "## ğŸš€ Main Data Collection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data collection loop\n",
    "all_usa_events = existing_data.copy()\n",
    "\n",
    "log_message(\"ğŸ‡ºğŸ‡¸ Starting 10-year USA financial data collection...\")\n",
    "log_message(f\"ğŸ“… Period: {START_DATE} to {END_DATE}\")\n",
    "log_message(f\"ğŸ“Š Total periods to collect: {len(monthly_ranges)}\")\n",
    "\n",
    "try:\n",
    "    for i, month_start in enumerate(monthly_ranges[resume_from:], start=resume_from):\n",
    "        # Calculate period end (end of month)\n",
    "        if month_start.month == 12:\n",
    "            month_end = month_start.replace(year=month_start.year + 1, month=1, day=1) - timedelta(days=1)\n",
    "        else:\n",
    "            month_end = month_start.replace(month=month_start.month + 1, day=1) - timedelta(days=1)\n",
    "        \n",
    "        month_start_str = month_start.strftime('%Y-%m-%d')\n",
    "        month_end_str = month_end.strftime('%Y-%m-%d')\n",
    "        \n",
    "        log_message(f\"ğŸ“… Period {i+1}/{len(monthly_ranges)}: {month_start_str} to {month_end_str}\")\n",
    "        \n",
    "        try:\n",
    "            # Search GDELT for the current month\n",
    "            results_df = gd.Search(\n",
    "                date=[month_start_str, month_end_str],\n",
    "                table='gkg',\n",
    "                output='df'\n",
    "            )\n",
    "            \n",
    "            if not results_df.empty:\n",
    "                raw_count = len(results_df)\n",
    "                collection_stats['total_raw_events'] += raw_count\n",
    "                \n",
    "                # Apply USA location filter first (most restrictive)\n",
    "                usa_location_filter = results_df['Locations'].str.contains(\n",
    "                    usa_location_pattern, case=False, na=False\n",
    "                )\n",
    "                usa_located = results_df[usa_location_filter].copy()\n",
    "                \n",
    "                if not usa_located.empty:\n",
    "                    # Apply USA financial theme filter\n",
    "                    usa_theme_filter = usa_located['Themes'].str.contains(\n",
    "                        usa_theme_pattern, case=False, na=False, regex=True\n",
    "                    )\n",
    "                    usa_themed = usa_located[usa_theme_filter].copy()\n",
    "                    \n",
    "                    if not usa_themed.empty:\n",
    "                        # Apply USA source filter\n",
    "                        usa_source_filter = usa_themed['SourceCommonName'].str.contains(\n",
    "                            usa_source_pattern, case=False, na=False\n",
    "                        )\n",
    "                        usa_final = usa_themed[usa_source_filter].copy()\n",
    "                        \n",
    "                        if not usa_final.empty:\n",
    "                            usa_count = len(usa_final)\n",
    "                            collection_stats['total_usa_events'] += usa_count\n",
    "                            all_usa_events.append(usa_final)\n",
    "                            \n",
    "                            log_message(f\"  âœ… Raw: {raw_count} â†’ USA Financial: {usa_count} events\")\n",
    "                        else:\n",
    "                            log_message(f\"  ğŸ“° No events after USA source filtering\")\n",
    "                    else:\n",
    "                        log_message(f\"  ğŸ·ï¸ No events after USA theme filtering\")\n",
    "                else:\n",
    "                    log_message(f\"  ğŸ“ No USA located events\")\n",
    "            else:\n",
    "                log_message(f\"  âŒ No events found for this period\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_message(f\"  âš ï¸ Error collecting data for this period: {e}\")\n",
    "            collection_stats['failed_periods'] += 1\n",
    "            continue\n",
    "        \n",
    "        collection_stats['completed_periods'] += 1\n",
    "        \n",
    "        # Save progress every 6 months\n",
    "        if (i + 1) % 6 == 0 and all_usa_events:\n",
    "            log_message(f\"ğŸ’¾ Saving progress at {month_start.year}...\")\n",
    "            temp_df = pd.concat(all_usa_events, ignore_index=True)\n",
    "            temp_df.to_csv(raw_data_file, index=False)\n",
    "            save_progress(collection_stats, collection_stats['completed_periods'])\n",
    "            log_message(f\"   Saved {len(temp_df)} total USA events so far\")\n",
    "        \n",
    "        # Be respectful to GDELT servers\n",
    "        time.sleep(4)  # 4 seconds between requests for Colab\n",
    "        \n",
    "        # Progress update every 3 months\n",
    "        if (i + 1) % 3 == 0:\n",
    "            elapsed_hours = (datetime.now() - pd.to_datetime(collection_stats['start_time'])).total_seconds() / 3600\n",
    "            remaining_periods = len(monthly_ranges) - collection_stats['completed_periods']\n",
    "            estimated_remaining = (remaining_periods * elapsed_hours) / max(collection_stats['completed_periods'], 1)\n",
    "            \n",
    "            log_message(f\"ğŸ“Š Progress Update:\")\n",
    "            log_message(f\"   Completed: {collection_stats['completed_periods']}/{collection_stats['total_periods']} periods\")\n",
    "            log_message(f\"   USA Events Collected: {collection_stats['total_usa_events']}\")\n",
    "            log_message(f\"   Failed Periods: {collection_stats['failed_periods']}\")\n",
    "            log_message(f\"   Elapsed Time: {elapsed_hours:.1f} hours\")\n",
    "            log_message(f\"   Estimated Remaining: {estimated_remaining:.1f} hours\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    log_message(\"â¹ï¸ Collection interrupted by user\")\n",
    "except Exception as e:\n",
    "    log_message(f\"ğŸ’¥ Unexpected error: {e}\")\n",
    "\n",
    "log_message(\"ğŸ Collection phase complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be358c",
   "metadata": {},
   "source": [
    "## ğŸ¯ Final Data Processing & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data processing\n",
    "log_message(\"ğŸ¯ Starting final data processing...\")\n",
    "\n",
    "if all_usa_events:\n",
    "    # Combine all collected data\n",
    "    final_usa_df = pd.concat(all_usa_events, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_count = len(final_usa_df)\n",
    "    final_usa_df = final_usa_df.drop_duplicates(subset=['DATE', 'DocumentIdentifier'])\n",
    "    duplicates_removed = initial_count - len(final_usa_df)\n",
    "    \n",
    "    log_message(f\"ğŸ”„ Removed {duplicates_removed} duplicate events\")\n",
    "    \n",
    "    # Parse dates and add time features\n",
    "    final_usa_df['date_parsed'] = pd.to_datetime(final_usa_df['DATE'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "    final_usa_df['date_only'] = final_usa_df['date_parsed'].dt.date\n",
    "    final_usa_df['year'] = final_usa_df['date_parsed'].dt.year\n",
    "    final_usa_df['month'] = final_usa_df['date_parsed'].dt.month\n",
    "    final_usa_df['weekday'] = final_usa_df['date_parsed'].dt.weekday\n",
    "    final_usa_df['hour'] = final_usa_df['date_parsed'].dt.hour\n",
    "    \n",
    "    # Parse sentiment scores\n",
    "    log_message(\"ğŸ“Š Parsing sentiment scores...\")\n",
    "    sentiment_data = final_usa_df['V2Tone'].apply(parse_v2tone_quick)\n",
    "    sentiment_df = pd.json_normalize(sentiment_data)\n",
    "    \n",
    "    for col in sentiment_df.columns:\n",
    "        final_usa_df[col] = sentiment_df[col].values\n",
    "    \n",
    "    # Save final processed data\n",
    "    final_usa_df.to_csv(raw_data_file, index=False)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    log_message(\"ğŸ“ˆ Generating summary statistics...\")\n",
    "    \n",
    "    # Yearly breakdown\n",
    "    yearly_counts = final_usa_df.groupby('year').size()\n",
    "    monthly_avg = final_usa_df.groupby(['year', 'month']).size().groupby('year').mean()\n",
    "    \n",
    "    # Sentiment statistics\n",
    "    sentiment_stats = final_usa_df['tone_avg'].describe()\n",
    "    \n",
    "    # Source breakdown\n",
    "    top_sources = final_usa_df['SourceCommonName'].value_counts().head(10)\n",
    "    \n",
    "    # Save summary report\n",
    "    summary_file = os.path.join(project_folder, 'collection_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"ğŸ‡ºğŸ‡¸ USA Financial Data Collection - Final Report\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Collection Period: {START_DATE} to {END_DATE}\\n\")\n",
    "        f.write(f\"Total Events Collected: {len(final_usa_df):,}\\n\")\n",
    "        f.write(f\"Average Events per Year: {len(final_usa_df) / len(yearly_counts):.0f}\\n\")\n",
    "        f.write(f\"Date Range in Data: {final_usa_df['date_only'].min()} to {final_usa_df['date_only'].max()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Yearly Distribution:\\n\")\n",
    "        for year, count in yearly_counts.items():\n",
    "            f.write(f\"  {year}: {count:,} events\\n\")\n",
    "        \n",
    "        f.write(f\"\\nSentiment Statistics:\\n\")\n",
    "        f.write(f\"  Mean: {sentiment_stats['mean']:.2f}\\n\")\n",
    "        f.write(f\"  Std: {sentiment_stats['std']:.2f}\\n\")\n",
    "        f.write(f\"  Min: {sentiment_stats['min']:.2f}\\n\")\n",
    "        f.write(f\"  Max: {sentiment_stats['max']:.2f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTop 10 News Sources:\\n\")\n",
    "        for source, count in top_sources.items():\n",
    "            f.write(f\"  {source}: {count:,} events\\n\")\n",
    "    \n",
    "    # Final statistics\n",
    "    collection_stats['end_time'] = datetime.now().isoformat()\n",
    "    collection_stats['final_event_count'] = len(final_usa_df)\n",
    "    collection_stats['duplicates_removed'] = duplicates_removed\n",
    "    \n",
    "    # Calculate total runtime\n",
    "    start_time = pd.to_datetime(collection_stats['start_time'])\n",
    "    end_time = pd.to_datetime(collection_stats['end_time'])\n",
    "    total_hours = (end_time - start_time).total_seconds() / 3600\n",
    "    \n",
    "    log_message(\"\\n\" + \"=\" * 50)\n",
    "    log_message(\"ğŸ† COLLECTION COMPLETE - FINAL RESULTS\")\n",
    "    log_message(\"=\" * 50)\n",
    "    log_message(f\"ğŸ“Š Total USA Financial Events: {len(final_usa_df):,}\")\n",
    "    log_message(f\"ğŸ“… Date Range: {final_usa_df['date_only'].min()} to {final_usa_df['date_only'].max()}\")\n",
    "    log_message(f\"â±ï¸ Total Runtime: {total_hours:.2f} hours\")\n",
    "    log_message(f\"ğŸ“ˆ Average Events per Year: {len(final_usa_df) / len(yearly_counts):.0f}\")\n",
    "    log_message(f\"ğŸ’¾ Data saved to: {raw_data_file}\")\n",
    "    log_message(f\"ğŸ“‹ Summary saved to: {summary_file}\")\n",
    "    \n",
    "    log_message(\"\\nğŸ“… Yearly Distribution:\")\n",
    "    for year, count in yearly_counts.items():\n",
    "        log_message(f\"   {year}: {count:,} events\")\n",
    "    \n",
    "    log_message(\"\\nâœ… Ready for financial forecasting model development!\")\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(collection_stats, collection_stats['completed_periods'])\n",
    "    \n",
    "else:\n",
    "    log_message(\"âŒ No data collected - check your filters and API connectivity\")\n",
    "\n",
    "log_message(\"ğŸ¯ Final processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f717e1",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download files to your local machine\n",
    "# (Data is already saved to Google Drive)\n",
    "\n",
    "try:\n",
    "    if os.path.exists(raw_data_file):\n",
    "        log_message(\"ğŸ“¥ Downloading data file...\")\n",
    "        files.download(raw_data_file)\n",
    "    \n",
    "    if os.path.exists(summary_file):\n",
    "        log_message(\"ğŸ“¥ Downloading summary file...\")\n",
    "        files.download(summary_file)\n",
    "    \n",
    "    if os.path.exists(log_file):\n",
    "        log_message(\"ğŸ“¥ Downloading log file...\")\n",
    "        files.download(log_file)\n",
    "        \n",
    "    log_message(\"âœ… Download complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log_message(f\"âš ï¸ Download failed: {e}\")\n",
    "    log_message(\"ğŸ’¡ Files are still available in Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e8038",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Instructions for Use\n",
    "\n",
    "### ğŸš€ **To Run This Notebook**:\n",
    "1. **Run all cells in order** (Ctrl+F9 or Runtime â†’ Run all)\n",
    "2. **Authorize Google Drive** when prompted\n",
    "3. **Keep the browser tab open** (the keep-alive function will help)\n",
    "4. **Check progress** in the Google Drive folder periodically\n",
    "\n",
    "### ğŸ”„ **If Interrupted**:\n",
    "- Simply re-run the notebook\n",
    "- It will automatically resume from where it left off\n",
    "- Progress is saved every 6 months\n",
    "\n",
    "### ğŸ“ **Output Files** (in Google Drive):\n",
    "- `gdelt_usa_10year_raw.csv` - Main dataset\n",
    "- `collection_summary.txt` - Statistics and summary\n",
    "- `collection_log.txt` - Detailed logs\n",
    "- `collection_progress.json` - Progress tracking\n",
    "\n",
    "### â±ï¸ **Expected Timeline**:\n",
    "- **Setup**: 2-3 minutes\n",
    "- **Data Collection**: 4-6 hours\n",
    "- **Final Processing**: 5-10 minutes\n",
    "- **Total**: ~5-7 hours\n",
    "\n",
    "### ğŸ¯ **Expected Results**:\n",
    "- ~5,000 high-quality USA financial events\n",
    "- Perfect for S&P 500, NASDAQ forecasting\n",
    "- Ready for machine learning models\n",
    "\n",
    "### ğŸ’¡ **Tips**:\n",
    "- **Colab Pro** recommended for longer runtimes\n",
    "- Can run overnight without your laptop\n",
    "- Files automatically sync to Google Drive\n",
    "- Resume capability if connection drops"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
